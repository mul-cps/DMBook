{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Analyzing Real-World Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following practical examples will showcase how outliers can be detected, visualized and how feature correlations can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_toc\">Content</a>\n",
    "\n",
    "[Learning Objectives](#sec_0)\n",
    "\n",
    "[a) Computation of Basic Statistics and Z-score method to detect outliers](#sec_a)\n",
    "\n",
    "[b) Analysis of Feature Correlation](#sec_b)\n",
    "\n",
    "[c) Creation of Representative Training and Test Datasets and Different Similarity Measures](#sec_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec_0\">Learning Objectives</a>\n",
    "\n",
    "* Understand the Z-score method to detect outliers.\n",
    "* Apply the threshold value and how it incluences outlier detection.\n",
    "* Interpret what a correlation matrix reveals about relationships in the data. \n",
    "* Know the difference between correlation and causality. \n",
    "* Understand k-means clustering.\n",
    "* Apply the Silhouette score and similarity measures, such as Kullback-Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE NEEDED LIBRARIES - DO NOT CHANGE\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_a\">a) Computation of Basic Statistics and Z-score method to detect outliers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers the computation of basic statistics and a common method for outlier detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Measures: Mean, Variance, Standard Deviation, and Median\n",
    "\n",
    "For the following statistics we assume that the underlying distributions are gaussian distributions. \n",
    "\n",
    "Below are the formulas for some key statistical measures that help us understand the characteristics of a dataset.\n",
    "\n",
    "1. **Mean**: The average value of a dataset, calculated by summing all values and dividing by the total count.\n",
    "\n",
    "   $$\n",
    "   \\text{Mean} = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "   $$\n",
    "\n",
    "2. **Variance**: A measure of how spread out the values in a dataset are, showing the average squared deviation from the mean.\n",
    "\n",
    "   $$\n",
    "   \\text{Variance} = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\text{Mean})^2\n",
    "   $$\n",
    "\n",
    "3. **Standard Deviation**: The square root of the variance, providing a measure of spread in the same units as the data.\n",
    "\n",
    "   $$\n",
    "   \\text{Standard Deviation} = {\\sigma} = \\sqrt{\\text{Variance}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\text{Mean})^2}\n",
    "   $$\n",
    "\n",
    "4. **Median**: The middle value of a sorted dataset, which separates the dataset into two equal halves.\n",
    "\n",
    "   - **If $ N $ (number of observations) is odd**:\n",
    "  \n",
    "     $$\n",
    "     \\text{Median} = x_{\\left(\\frac{N+1}{2}\\right)}\n",
    "     $$\n",
    "     \n",
    "\n",
    "   - **If $ N $ is even**:\n",
    "  \n",
    "     $$\n",
    "     \\text{Median} = \\frac{x_{\\left(\\frac{N}{2}\\right)} + x_{\\left(\\frac{N}{2} + 1\\right)}}{2}\n",
    "     $$\n",
    "     \n",
    "\n",
    "These measures provide insights into the central tendency (mean, median) and the spread (variance, standard deviation) of a dataset.\n",
    "\n",
    "### Z-score Outlier Detection\n",
    "\n",
    "Z-score outlier detection identifies data points that deviate significantly from the mean by measuring how many standard deviations away they are. The Z-score is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Z-score} = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "A common threshold (e.g., $|Z| > 3$) flags points as outliers if they lie far from the mean, helping to detect anomalies in normally distributed data.\n",
    "\n",
    "Note that the Z-score method will sometimes lead to the exclusion of data points that are actually critical for preserving the integrity and representativeness of the dataset. The primary goal of the Z-score method is to remove erroneous or anomalous data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def read_file_into_dataset(filename):\n",
    "    # Read the CSV file with pandas, keeping headers\n",
    "    dataset = pd.read_csv(filename, encoding=\"utf-8\")\n",
    "    return dataset\n",
    "\n",
    "def print_all_cities_with_mapping(dataset):\n",
    "    \"\"\"\n",
    "    Prints all unique city names from the dataset and assigns a unique number to each city.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the original dataset (no cleaning or processing applied).\n",
    "    \n",
    "    Returns:\n",
    "    - city_mapping: dict, a mapping of city names to unique numbers.\n",
    "    \"\"\"\n",
    "    if 'City' not in dataset.columns:\n",
    "        raise ValueError(\"The dataset does not contain a 'City' column.\")\n",
    "    \n",
    "    # Get the unique city names\n",
    "    unique_cities = dataset['City'].unique()\n",
    "    \n",
    "    # Create a mapping of city names to unique numbers\n",
    "    city_mapping = {city: idx for idx, city in enumerate(unique_cities)}\n",
    "    \n",
    "    print(\"List of all cities in the dataset and their mappings:\")\n",
    "    for city, number in city_mapping.items():\n",
    "        print(f\"{number}: {city}\")\n",
    "    \n",
    "    return city_mapping\n",
    "\n",
    "# Specific for \"UrbanAirQualityandHealthImpactDataset.csv\"\n",
    "def clean_dataframe(dataframe, columns_to_remove):\n",
    "    # Drop specified columns\n",
    "    df_cleaned = dataframe.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    # Convert boolean columns to 0 and 1\n",
    "    boolean_columns = df_cleaned.select_dtypes(include='bool').columns\n",
    "    df_cleaned[boolean_columns] = df_cleaned[boolean_columns].astype(int)\n",
    "    \n",
    "    # Encode City column to numerical values starting from 1\n",
    "    city_codes, unique_cities = pd.factorize(df_cleaned['City'])\n",
    "    df_cleaned['City'] = city_codes + 1  # Adjust index to start from 1\n",
    "    \n",
    "    # Create dictionary with city names as keys and their corresponding numerical values as values\n",
    "    city_mapping = {unique_cities[i]: i + 1 for i in range(len(unique_cities))}\n",
    "    \n",
    "    return df_cleaned, city_mapping\n",
    "\n",
    "def filter_numeric_columns_and_cities(dataset, cities_to_keep=None, city_mapping=None, keep_all_cities=False):\n",
    "    \"\"\"\n",
    "    Filters the dataset to include only numeric columns and rows corresponding to specified cities, \n",
    "    or all cities if `keep_all_cities` is True. Always keeps the city labels as per the provided `city_mapping`.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to filter.\n",
    "    - cities_to_keep: list, a list of city names to keep (ignored if `keep_all_cities` is True).\n",
    "    - city_mapping: dict, mapping of city names to unique numeric labels.\n",
    "    - keep_all_cities: bool, whether to keep all cities (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - numeric_dataset: DataFrame, a numeric dataset with city labels processed as per `city_mapping`.\n",
    "    \"\"\"\n",
    "    if city_mapping is None:\n",
    "        raise ValueError(\"A valid city_mapping must be provided.\")\n",
    "\n",
    "    # Ensure the dataset has a valid 'City' column\n",
    "    if 'City' not in dataset.columns:\n",
    "        raise ValueError(\"The dataset does not contain a 'City' column.\")\n",
    "\n",
    "    dataset = dataset.copy()  # Explicitly create a copy to avoid warnings\n",
    "\n",
    "    # Keep all cities if specified\n",
    "    if keep_all_cities:\n",
    "        dataset['City'] = dataset['City'].map(city_mapping)\n",
    "    else:\n",
    "        # Filter the dataset to keep only the specified cities\n",
    "        if not cities_to_keep:\n",
    "            raise ValueError(\"Please provide city names to keep when `keep_all_cities` is False.\")\n",
    "        dataset = dataset[dataset['City'].isin(cities_to_keep)].copy()\n",
    "        dataset['City'] = dataset['City'].map(city_mapping)\n",
    "\n",
    "    # Check for unmapped cities\n",
    "    if dataset['City'].isna().any():\n",
    "        unmapped_cities = dataset.loc[dataset['City'].isna(), 'City'].unique()\n",
    "        raise ValueError(f\"The following cities were not found in the city_mapping: {unmapped_cities}\")\n",
    "\n",
    "    # Attempt to convert all columns to numeric, forcing non-convertible values to NaN\n",
    "    dataset_converted = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop columns that contain NaN (these had non-numeric entries)\n",
    "    numeric_dataset = dataset_converted.dropna(axis=1)\n",
    "\n",
    "    # Downcast remaining values to appropriate numeric types (float or int)\n",
    "    numeric_dataset = numeric_dataset.apply(pd.to_numeric, downcast='float')\n",
    "\n",
    "    return numeric_dataset\n",
    "\n",
    "\n",
    "\n",
    "def compute_basic_statistics(dataset):\n",
    "    stats = {\n",
    "        \"mean\": dataset.mean(),\n",
    "        \"std_dev\": dataset.std(),\n",
    "        \"variance\": dataset.var(),\n",
    "        \"median\": dataset.median()\n",
    "    }\n",
    "    # Convert to DataFrame and format with no scientific notation\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    pd.options.display.float_format = '{:,.2f}'.format  # Set global display option for floating-point numbers\n",
    "    return stats_df\n",
    "\n",
    "def reduce_to_selected_columns(dataset, columns_to_keep=None):\n",
    "    \"\"\"\n",
    "    Reduces the dataset to only the specified columns. \n",
    "    If no columns are specified, it defaults to the first two columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to reduce.\n",
    "    - columns_to_keep: list of str, the column headers to keep (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - reduced_dataset: DataFrame, the dataset with only the specified columns.\n",
    "    \"\"\"\n",
    "    if columns_to_keep is not None:\n",
    "        # Keep only the columns specified in columns_to_keep\n",
    "        reduced_dataset = dataset[columns_to_keep]\n",
    "    else:\n",
    "        # Default to the first two columns if no columns are specified\n",
    "        reduced_dataset = dataset.iloc[:, :2]\n",
    "    \n",
    "    return reduced_dataset\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed=42):\n",
    "    # Shuffle the dataset rows with a fixed random seed\n",
    "    shuffled_dataset = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return shuffled_dataset\n",
    "\n",
    "def detect_outliers_z_score(dataset, threshold=3):\n",
    "    \"\"\"\n",
    "    Detects outliers in the dataset based on the Z-score method.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to analyze.\n",
    "    - threshold: float, the Z-score threshold to identify outliers (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    - outliers: DataFrame, a boolean DataFrame indicating outliers (True for outliers).\n",
    "    \"\"\"\n",
    "    # Calculate Z-scores for each numeric column\n",
    "    z_scores = dataset.apply(zscore)\n",
    "    \n",
    "    # Identify outliers where |Z| > threshold\n",
    "    outliers = (z_scores.abs() > threshold)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def remove_outliers(dataset, outliers):\n",
    "    \"\"\"\n",
    "    Removes rows from the dataset where any feature is marked as an outlier.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to remove outliers from.\n",
    "    - outliers: pandas DataFrame, a boolean DataFrame indicating outliers (True for outliers).\n",
    "\n",
    "    Returns:\n",
    "    - cleaned_dataset: DataFrame, the dataset with outliers removed.\n",
    "    \"\"\"\n",
    "    # Identify rows where any column is marked as an outlier\n",
    "    rows_with_outliers = outliers.any(axis=1)\n",
    "    \n",
    "    # Remove these rows from the dataset\n",
    "    cleaned_dataset = dataset[~rows_with_outliers].reset_index(drop=True)\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "def visualize_data_with_outliers_histogram(dataset, outliers, selected_columns):\n",
    "    # Create a 1x2 figure for the histograms\n",
    "    fig, axs = plt.subplots(1, len(selected_columns), figsize=(14, 6))\n",
    "    fig.suptitle(\"Histogram of Data with Outliers Highlighted\", fontsize=16)\n",
    "    \n",
    "    # Plot histograms for each selected column\n",
    "    for i, column in enumerate(selected_columns):\n",
    "        bins = 20\n",
    "        data_min, data_max = dataset[column].min(), dataset[column].max()\n",
    "        bin_edges = np.linspace(data_min, data_max, bins + 1)\n",
    "        \n",
    "        # Histogram for the full data (blue) and outliers (red)\n",
    "        axs[i].hist(dataset[column], bins=bin_edges, color='blue', alpha=0.7, label='Data')\n",
    "        axs[i].hist(dataset[column][outliers[column]], bins=bin_edges, color='red', alpha=0.7, label='Outliers')\n",
    "        \n",
    "        axs[i].set_xlabel(column)\n",
    "        axs[i].set_ylabel(\"Frequency\")\n",
    "        axs[i].set_title(f\"Histogram of {column} with Outliers Highlighted\")\n",
    "        axs[i].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit title and avoid overlapping\n",
    "    plt.show()\n",
    "\n",
    "def calculate_whis_for_zscore(dataset, column, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Calculate the equivalent 'whis' parameter for the IQR to align with a given Z-score threshold.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (DataFrame): The dataset containing the data.\n",
    "        column (str): The column for which to calculate the whis value.\n",
    "        threshold (float): The Z-score threshold.\n",
    "\n",
    "    Returns:\n",
    "        float: The equivalent whis parameter for the boxplot.\n",
    "    \"\"\"\n",
    "    q1 = dataset[column].quantile(0.25)\n",
    "    q3 = dataset[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    std_dev = dataset[column].std()\n",
    "\n",
    "    # Convert Z-score threshold to an IQR multiplier\n",
    "    whis_equivalent = (threshold * std_dev) / iqr\n",
    "    return float(whis_equivalent)\n",
    "\n",
    "def visualize_data_with_outliers_boxplot_aligned(dataset, outliers, selected_columns, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Visualizes the dataset with boxplots for selected columns, highlighting outliers based on a Z-score threshold.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (DataFrame): The dataset to visualize.\n",
    "        outliers (dict): A dictionary indicating outliers for each column.\n",
    "        selected_columns (list): Columns to include in the visualization.\n",
    "        threshold (float): The Z-score threshold for outlier detection.\n",
    "    \"\"\"\n",
    "    # Create a figure for boxplots of the selected columns\n",
    "    fig, axs = plt.subplots(1, len(selected_columns), figsize=(12, 6))\n",
    "    fig.suptitle(\"Boxplot of Data with Outliers Highlighted\", fontsize=16)\n",
    "\n",
    "    for i, column in enumerate(selected_columns):\n",
    "        # Calculate the whis equivalent for the Z-score threshold\n",
    "        whis = calculate_whis_for_zscore(dataset, column, threshold=threshold)\n",
    "        \n",
    "        # Plot the boxplot using the dynamically calculated whis\n",
    "        sns.boxplot(data=dataset, x=column, ax=axs[i], color=\"lightblue\", fliersize=5, whis=whis)\n",
    "        \n",
    "        # Customize plot aesthetics\n",
    "        axs[i].set_title(f\"Boxplot of {column} with Outliers (Threshold: |Z| > {threshold})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_data_with_outliers_violin(dataset, outliers, selected_columns):\n",
    "    # Create a figure for violin plots of the selected columns\n",
    "    fig, axs = plt.subplots(1, len(selected_columns), figsize=(12, 6))\n",
    "    fig.suptitle(\"Violin Plot of Data with Outliers Highlighted\", fontsize=16)\n",
    "\n",
    "    for i, column in enumerate(selected_columns):\n",
    "        # Violin plot for the main data\n",
    "        sns.violinplot(y=dataset[column], ax=axs[i], color=\"lightblue\", inner=\"quartile\")\n",
    "\n",
    "        # Plot outliers on a single horizontal line below the violin plot\n",
    "        outlier_values = dataset[column][outliers[column]]\n",
    "        axs[i].scatter(np.zeros_like(outlier_values), outlier_values, color='red', marker='x', label='Outliers')\n",
    "\n",
    "        axs[i].set_ylim(bottom=-1)  # Extend y-axis to make space for the outlier line\n",
    "        axs[i].set_title(f\"Violin Plot of {column} with Outliers Highlighted\")\n",
    "\n",
    "        # Custom legend\n",
    "        data_patch = mpatches.Patch(color='lightblue', label='Data')\n",
    "        outlier_patch = mpatches.Patch(color='red', label='Outliers')\n",
    "        axs[i].legend(handles=[data_patch, outlier_patch])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_value(x, name):\n",
    "    if x is None:\n",
    "        print(f\"{name} is still None. Please enter a valid value for {name}.\")\n",
    "        raise Exception(f\"Execution stopped because {name} is not defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "path = \"urban_air_quality_and_health.csv\"\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "city_mapping = print_all_cities_with_mapping(dataset) # Print all cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can decide two cities which will be used for some of the remaining computations of this notebook. \\\n",
    "If you want to change them, always do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city1 = None      # Change None to any of the printed city names\n",
    "city2 = None      # Change None to any of the printed city names\n",
    "\n",
    "## DO NOT CHANGE THE CODE BELOW\n",
    "check_value(city1, \"city1\")\n",
    "check_value(city2, \"city2\")\n",
    "cities_to_keep = [city1, city2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next codeblock removes columns that are unfit for computations (Strings) and computes and prints some basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Remove non-relevant columns\n",
    "columns_to_remove = [\n",
    "    'datetime', 'preciptype', 'sunrise', 'sunset', 'conditions', 'description', \n",
    "    'icon', 'stations', 'source', 'Season', 'Day_of_Week', 'precip', 'precipprob', \n",
    "    'precipcover', 'moonphase'\n",
    "]\n",
    "dataset_cleaned = dataset.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "# Filter dataset to numeric columns and specified cities\n",
    "allcities_numeric_dataset = filter_numeric_columns_and_cities(\n",
    "    dataset_cleaned,\n",
    "    keep_all_cities=True,\n",
    "    city_mapping=city_mapping\n",
    ")\n",
    "\n",
    "numeric_dataset = filter_numeric_columns_and_cities(\n",
    "    dataset_cleaned,\n",
    "    cities_to_keep=cities_to_keep,\n",
    "    city_mapping=city_mapping\n",
    ")\n",
    "\n",
    "# Compute basic statistics\n",
    "stats = compute_basic_statistics(numeric_dataset)\n",
    "print(\"Basic Statistics:\")\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can decide two columns for the plots and computations within the entire notebook. \\\n",
    "If you want to change them, please always do so here and rerun the program from this cell. \\\n",
    "The column city will always be kept, so do not name neither 'col1' nor 'col2' 'City'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = None    # Change None to any column included in the rows of basic statistics, e.g. \"tempmax\", \"tempmin\", or \"humidity\"\n",
    "col2 = None    # Change None to any column included in the rows of basic statistics, e.g. \"pressure\", \"windgust\", or \"windspeed\"\n",
    "\n",
    "## DO NOT CHANGE THE CODE BELOW\n",
    "check_value(col1, \"col1\")\n",
    "check_value(col2, \"col2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can change the threshold for the z-score outlier detection \\\n",
    "for the remaining computations in chapter a) and b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = None  # CHANGE threshold from None to any positive number, default value is usually 3.0\n",
    "\n",
    "## DO NOT CHANGE THE CODE BELOW\n",
    "check_value(threshold, \"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block plots and visualizes the distributions and its outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Reduce to selected columns\n",
    "reduced_allcities_numeric_dataset = reduce_to_selected_columns(allcities_numeric_dataset, ['City', col1, col2])\n",
    "\n",
    "# Shuffle dataset (not necessary)\n",
    "shuffled_reduced_allcities_numeric_dataset = shuffle_dataset(reduced_allcities_numeric_dataset)\n",
    "\n",
    "# Compute and print the outliers\n",
    "outliers = detect_outliers_z_score(shuffled_reduced_allcities_numeric_dataset, threshold=threshold)\n",
    "print(\"Outliers Detected:\")\n",
    "print(outliers)\n",
    "\n",
    "# Plot the outliers in a histogram, boxplot, and violin plot\n",
    "selected_columns = [col1, col2]\n",
    "visualize_data_with_outliers_histogram(shuffled_reduced_allcities_numeric_dataset, outliers, selected_columns)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "visualize_data_with_outliers_boxplot_aligned(\n",
    "    shuffled_reduced_allcities_numeric_dataset,\n",
    "    outliers,\n",
    "    selected_columns,\n",
    "    threshold=threshold\n",
    ")\n",
    "visualize_data_with_outliers_violin(shuffled_reduced_allcities_numeric_dataset, outliers, selected_columns)\n",
    "\n",
    "# Remove outliers\n",
    "cleaned_numeric_dataset = remove_outliers(shuffled_reduced_allcities_numeric_dataset, outliers)\n",
    "\n",
    "# Final dataset verification\n",
    "print(\"Cleaned Numeric Dataset:\")\n",
    "print(cleaned_numeric_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_b\">b) Analysis of Feature Correlation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will have a look at how much the different features correlate. \\\n",
    "We will also classify the data with the k-Means clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Correlation Matrix\n",
    "\n",
    "**Correlation** is a statistical measure that quantifies the strength and direction of a linear relationship between two variables. It is expressed as a value between $-1$ and $+1$, where:\n",
    "- $+1$: Perfect positive correlation, indicating that as one variable increases, the other increases proportionally.\n",
    "- $0$: No linear relationship between the variables.\n",
    "- $-1$: Perfect negative correlation, indicating that as one variable increases, the other decreases proportionally.\n",
    "\n",
    "It is important to note that correlation does not imply causation. A correlation between two variables means that changes in one variable are statistically associated with changes in the other, but it does not mean that one variable directly causes the other. Instead, the probability of one variable occurring might be higher when the other variable changes.\n",
    "\n",
    "A **correlation matrix** is a table showing the correlation coefficients between variables in a dataset, providing a compact representation of their pairwise relationships. Each cell in the matrix contains the correlation value for a specific pair of variables, helping identify trends or dependencies within the data.\n",
    "\n",
    "### Silhouette Score for k-Means Clustering\n",
    "\n",
    "The silhouette score measures how similar a data point is to its own cluster compared to other clusters, helping to assess the quality of clustering. It ranges from -1 to 1, where a high score (close to 1) indicates that the point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "To compute the optimal number of clusters k for k-means clustering, we can calculate the silhouette score for each k and choose the k that yields the highest average silhouette score across all data points.\n",
    "\n",
    "### Normalization for k-Means Clustering\n",
    "\n",
    "Normalization is the process of scaling data to a common range, typically between 0 and 1 or -1 and 1, to ensure that all features contribute equally in analyses.\n",
    "\n",
    "Normalization is important for K-means clustering because it ensures that each feature contributes equally to the distance calculations, preventing features with larger scales from dominating the clustering process. Without normalization, K-means may produce biased clusters that are heavily influenced by features with larger numerical ranges.\n",
    "\n",
    "$$\n",
    "  \\text{normalized features} = \\frac{{x - \\mu}}{{\\sigma}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def plot_correlation_matrix(dataset):\n",
    "    \"\"\"\n",
    "    Generates a heatmap to visualize correlations between numeric features.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = dataset.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def apply_kmeans_clustering(dataset, n_clusters=3, selected_columns=None, seed=42):\n",
    "    \"\"\"\n",
    "    Applies K-means clustering on selected features and visualizes the clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to perform clustering on.\n",
    "    - n_clusters: int, the number of clusters to form.\n",
    "    - selected_columns: list of str, the column names to use for clustering.\n",
    "    - seed: int, the random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - dataset_with_clusters: DataFrame, the dataset with an added 'Cluster' column.\n",
    "    \"\"\"\n",
    "    if selected_columns is None:\n",
    "        # Default to using the first two numeric columns if none are provided\n",
    "        selected_columns = dataset.select_dtypes(include=['float', 'int']).columns[:2].tolist()\n",
    "    \n",
    "    # Extract data for the selected columns\n",
    "    data_to_cluster = dataset[selected_columns]\n",
    "    \n",
    "    # Initialize and fit the KMeans model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=seed)\n",
    "    clusters = kmeans.fit_predict(data_to_cluster)\n",
    "    \n",
    "    # Add cluster labels to the dataset\n",
    "    dataset_with_clusters = dataset.copy()\n",
    "    dataset_with_clusters['Cluster'] = clusters\n",
    "    \n",
    "    # Visualize the clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data_to_cluster.iloc[:, 0], data_to_cluster.iloc[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.7)\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "    plt.xlabel(selected_columns[0])\n",
    "    plt.ylabel(selected_columns[1])\n",
    "    plt.title(f'K-Means Clustering on {selected_columns[0]} and {selected_columns[1]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return dataset_with_clusters\n",
    "\n",
    "def compute_silhouette_score_for_k(data, k, seed=42):\n",
    "    \"\"\"\n",
    "    Computes the silhouette score for a given k.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, data to cluster.\n",
    "    - k: int, number of clusters.\n",
    "    - seed: int, random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - silhouette score for the given k.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=seed)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    score = silhouette_score(data, labels)\n",
    "    return score\n",
    "\n",
    "def find_optimal_k_silhouette(dataset, selected_columns=None, seed=42, k_range=(2, 10)):\n",
    "    \"\"\"\n",
    "    Finds the optimal number of clusters using silhouette score.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to analyze.\n",
    "    - selected_columns: list of str, columns to use for clustering.\n",
    "    - seed: int, random seed for reproducibility.\n",
    "    - k_range: tuple, the range of k values to consider (inclusive).\n",
    "    \n",
    "    Returns:\n",
    "    - best_k: int, the optimal number of clusters.\n",
    "    \"\"\"\n",
    "    if selected_columns is None:\n",
    "        # Default to using the first two numeric columns if none are provided\n",
    "        selected_columns = dataset.select_dtypes(include=['float', 'int']).columns[:2].tolist()\n",
    "    \n",
    "    data_to_cluster = dataset[selected_columns]\n",
    "    \n",
    "    best_k = k_range[0]\n",
    "    best_score = -1\n",
    "    \n",
    "    for k in range(k_range[0], k_range[1] + 1):\n",
    "        score = compute_silhouette_score_for_k(data_to_cluster, k, seed)\n",
    "        print(f\"Silhouette score for k={k}: {score:.4f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "            \n",
    "    print(f\"Optimal number of clusters: {best_k} with silhouette score: {best_score:.4f}\")\n",
    "    return best_k\n",
    "\n",
    "def normalize_dataset(dataset, method='z-score'):\n",
    "    \"\"\"\n",
    "    Normalizes a pandas DataFrame containing numeric values, excluding the 'City' column.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pandas DataFrame, the dataset to normalize.\n",
    "    - method: str, the normalization method to use ('z-score' or 'min-max').\n",
    "\n",
    "    Returns:\n",
    "    - normalized_dataset: DataFrame, the normalized dataset with the 'City' column unchanged.\n",
    "    \"\"\"\n",
    "    # Initialize the scaler based on the selected method\n",
    "    if method == 'z-score':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'min-max':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'z-score' or 'min-max'\")\n",
    "\n",
    "    # Separate the 'City' column (if present) from the rest of the dataset\n",
    "    if 'City' in dataset.columns:\n",
    "        city_column = dataset['City']\n",
    "        dataset_to_normalize = dataset.drop(columns=['City'])\n",
    "    else:\n",
    "        city_column = None\n",
    "        dataset_to_normalize = dataset\n",
    "\n",
    "    # Apply the scaler to the numeric columns\n",
    "    normalized_values = scaler.fit_transform(dataset_to_normalize)\n",
    "    \n",
    "    # Convert the result back to a DataFrame with the original column names\n",
    "    normalized_dataset = pd.DataFrame(normalized_values, columns=dataset_to_normalize.columns)\n",
    "    \n",
    "    # Reattach the 'City' column if it was separated\n",
    "    if city_column is not None:\n",
    "        normalized_dataset['City'] = city_column.reset_index(drop=True)\n",
    "\n",
    "    return normalized_dataset\n",
    "\n",
    "\n",
    "def plot_two_correlation_matrices(dataset1, dataset2, selected_columns=None, title1=\"Correlation Matrix 1\", title2=\"Correlation Matrix 2\"):\n",
    "    \"\"\"\n",
    "    Plots two correlation matrices side by side, with the option to select specific columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset1: pandas DataFrame, first dataset for the correlation matrix.\n",
    "    - dataset2: pandas DataFrame, second dataset for the correlation matrix.\n",
    "    - selected_columns: list of str, column names to include in the correlation matrix.\n",
    "    - title1: str, title for the first correlation matrix.\n",
    "    - title2: str, title for the second correlation matrix.\n",
    "    \"\"\"\n",
    "    # If selected_columns is provided, filter datasets to include only those columns\n",
    "    if selected_columns:\n",
    "        dataset1 = dataset1[selected_columns]\n",
    "        dataset2 = dataset2[selected_columns]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Correlation matrix for the first dataset\n",
    "    corr_matrix1 = dataset1.corr()\n",
    "    sns.heatmap(corr_matrix1, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, ax=axes[0])\n",
    "    axes[0].set_title(title1)\n",
    "\n",
    "    # Correlation matrix for the second dataset\n",
    "    corr_matrix2 = dataset2.corr()\n",
    "    sns.heatmap(corr_matrix2, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, ax=axes[1])\n",
    "    axes[1].set_title(title2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_two_kmeans_clusters(dataset1, dataset2, n_clusters1=3, n_clusters2=3, \n",
    "                             selected_columns1=None, selected_columns2=None, seed=42):\n",
    "    \"\"\"\n",
    "    Plots two K-means clustering results side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset1: pandas DataFrame, first dataset for K-means clustering.\n",
    "    - dataset2: pandas DataFrame, second dataset for K-means clustering.\n",
    "    - n_clusters1: int, number of clusters for the first K-means plot.\n",
    "    - n_clusters2: int, number of clusters for the second K-means plot.\n",
    "    - selected_columns1: list of str, columns to use for clustering in the first plot.\n",
    "    - selected_columns2: list of str, columns to use for clustering in the second plot.\n",
    "    - seed: int, random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Default to first two numeric columns if not provided\n",
    "    if selected_columns1 is None:\n",
    "        selected_columns1 = dataset1.select_dtypes(include=['float', 'int']).columns[:2].tolist()\n",
    "    if selected_columns2 is None:\n",
    "        selected_columns2 = dataset2.select_dtypes(include=['float', 'int']).columns[:2].tolist()\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    data1 = dataset1[selected_columns1]\n",
    "    data2 = dataset2[selected_columns2]\n",
    "    \n",
    "    # Apply KMeans clustering on the first dataset\n",
    "    kmeans1 = KMeans(n_clusters=n_clusters1, random_state=seed)\n",
    "    clusters1 = kmeans1.fit_predict(data1)\n",
    "    \n",
    "    # Apply KMeans clustering on the second dataset\n",
    "    kmeans2 = KMeans(n_clusters=n_clusters2, random_state=seed)\n",
    "    clusters2 = kmeans2.fit_predict(data2)\n",
    "    \n",
    "    # Plotting side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # First clustering plot\n",
    "    axes[0].scatter(data1.iloc[:, 0], data1.iloc[:, 1], c=clusters1, cmap='viridis', s=50, alpha=0.7)\n",
    "    axes[0].scatter(kmeans1.cluster_centers_[:, 0], kmeans1.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "    axes[0].set_xlabel(selected_columns1[0])\n",
    "    axes[0].set_ylabel(selected_columns1[1])\n",
    "    axes[0].set_title(f'K-Means Clustering on {selected_columns1[0]} and {selected_columns1[1]} with k={n_clusters1}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Second clustering plot\n",
    "    axes[1].scatter(data2.iloc[:, 0], data2.iloc[:, 1], c=clusters2, cmap='viridis', s=50, alpha=0.7)\n",
    "    axes[1].scatter(kmeans2.cluster_centers_[:, 0], kmeans2.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "    axes[1].set_xlabel(selected_columns2[0])\n",
    "    axes[1].set_ylabel(selected_columns2[1])\n",
    "    axes[1].set_title(f'K-Means Clustering on {selected_columns2[0]} and {selected_columns2[1]} with k={n_clusters2}, without outliers')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_custom_scatter(dataset1, dataset2, plot_columns, city_mapping, cmap='viridis_r'):\n",
    "    \"\"\"\n",
    "    Plots two scatter plots side by side with specified columns for x, y, and city (color).\n",
    "\n",
    "    Parameters:\n",
    "    - dataset1: pandas DataFrame, first dataset to plot.\n",
    "    - dataset2: pandas DataFrame, second dataset to plot.\n",
    "    - plot_columns: list of str, [x, y] columns for the plots.\n",
    "    - city_mapping: dict, mapping of city names to unique numeric labels.\n",
    "    - cmap: str, color map to use for the scatter plots.\n",
    "    \"\"\"\n",
    "    # Unpack column names for x and y axes\n",
    "    x, y = plot_columns\n",
    "\n",
    "    # Reverse the city mapping to get city names from numeric labels\n",
    "    reverse_mapping = {v: k for k, v in city_mapping.items()}\n",
    "\n",
    "    # Ensure that only cities in the dataset are included in the legend and colorbar\n",
    "    unique_cities1 = dataset1['City'].unique()\n",
    "    unique_cities2 = dataset2['City'].unique()\n",
    "    all_unique_cities = sorted(set(unique_cities1).union(set(unique_cities2)))\n",
    "\n",
    "    # Check that all unique cities are in the mapping\n",
    "    valid_cities = [city for city in all_unique_cities if city in reverse_mapping]\n",
    "    if not valid_cities:\n",
    "        raise ValueError(\"No valid city mappings found for the provided datasets.\")\n",
    "\n",
    "    # Get the city names for the valid cities\n",
    "    city_names = [reverse_mapping[city] for city in valid_cities]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # First plot\n",
    "    scatter1 = axes[0].scatter(\n",
    "        dataset1[x], dataset1[y], c=dataset1['City'], cmap=cmap, s=50, alpha=0.7\n",
    "    )\n",
    "    axes[0].set_xlabel(x)\n",
    "    axes[0].set_ylabel(y)\n",
    "    axes[0].set_title(f'Scatter Plot of {x} vs {y} ({\", \".join(city_names)})')\n",
    "    colorbar1 = fig.colorbar(scatter1, ax=axes[0])\n",
    "    colorbar1.set_ticks(valid_cities)\n",
    "    colorbar1.set_ticklabels([reverse_mapping[city] for city in valid_cities])\n",
    "    colorbar1.set_label('City')\n",
    "\n",
    "    # Second plot\n",
    "    scatter2 = axes[1].scatter(\n",
    "        dataset2[x], dataset2[y], c=dataset2['City'], cmap=cmap, s=50, alpha=0.7\n",
    "    )\n",
    "    axes[1].set_xlabel(x)\n",
    "    axes[1].set_ylabel(y)\n",
    "    axes[1].set_title(f'Scatter Plot of {x} vs {y} ({\", \".join(city_names)} - without outliers)')\n",
    "    colorbar2 = fig.colorbar(scatter2, ax=axes[1])\n",
    "    colorbar2.set_ticks(valid_cities)\n",
    "    colorbar2.set_ticklabels([reverse_mapping[city] for city in valid_cities])\n",
    "    colorbar2.set_label('City')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>TASK:</u> How does the correlation matrix change with different thresholds? \\\n",
    "Compare the correlation matrices for threshold = [1, 3]. \\\n",
    "Why is the effect strongest at threshold = 1? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = None  # CHANGE threshold from None to any positive number, default value is usually 3.0\n",
    "\n",
    "## DO NOT CHANGE THE CODE BELOW\n",
    "check_value(threshold, \"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codeblock computes the correlation matrices, once with outliers and once without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "selected_columns_correlation_matrix = ['tempmax', 'tempmin', 'humidity', 'dew', 'pressure', 'windgust', 'windspeed', 'solarradiation', 'uvindex']\n",
    "selected_columns = [col1, col2]  \n",
    "\n",
    "# Compute and plot the correlation matrix without outliers\n",
    "outliers = detect_outliers_z_score(allcities_numeric_dataset, threshold=threshold)\n",
    "cleaned_allcities_numeric_dataset = remove_outliers(allcities_numeric_dataset, outliers)\n",
    "\n",
    "plot_two_correlation_matrices(\n",
    "    allcities_numeric_dataset,\n",
    "    cleaned_allcities_numeric_dataset,\n",
    "    selected_columns=selected_columns_correlation_matrix,\n",
    "    title1=\"All Cities with Outliers\",\n",
    "    title2=\"All Cities without Outliers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use K-means Clustering to try to distinguish between the two cities. The Silhouette Score is also computed to find out the theoretical optimal amount of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Reduce to selected columns\n",
    "reduced_numeric_dataset = reduce_to_selected_columns(numeric_dataset, ['City', col1, col2])\n",
    "\n",
    "# Shuffle dataset (not necessary)\n",
    "shuffled_reduced_numeric_dataset = shuffle_dataset(reduced_numeric_dataset)\n",
    "\n",
    "# Compute and plot the result of k-means clustering with outliers and without outliers\n",
    "# Normalize the shuffled reduced numeric dataset\n",
    "normalized_shuffled_reduced_numeric_dataset = normalize_dataset(shuffled_reduced_numeric_dataset)\n",
    "\n",
    "# Find optimal k for clustering before and after outlier removal\n",
    "optimal_k = find_optimal_k_silhouette(\n",
    "    normalized_shuffled_reduced_numeric_dataset,\n",
    "    selected_columns=selected_columns,\n",
    "    k_range=(2, 10)\n",
    ")\n",
    "outliers = detect_outliers_z_score(normalized_shuffled_reduced_numeric_dataset, threshold=threshold)\n",
    "cleaned_normalized_shuffled_reduced_numeric_dataset = remove_outliers(normalized_shuffled_reduced_numeric_dataset, outliers)\n",
    "\n",
    "cleaned_optimal_k = find_optimal_k_silhouette(\n",
    "    cleaned_normalized_shuffled_reduced_numeric_dataset,\n",
    "    selected_columns=selected_columns,\n",
    "    k_range=(2, 10)\n",
    ")\n",
    "\n",
    "# Plot K-means clustering results\n",
    "plot_two_kmeans_clusters(\n",
    "    normalized_shuffled_reduced_numeric_dataset,\n",
    "    cleaned_normalized_shuffled_reduced_numeric_dataset,\n",
    "    n_clusters1=optimal_k,\n",
    "    n_clusters2=cleaned_optimal_k,\n",
    "    selected_columns1=selected_columns,\n",
    "    selected_columns2=selected_columns\n",
    ")\n",
    "\n",
    "# Compare clustering results with fixed k=2\n",
    "plot_two_kmeans_clusters(\n",
    "    normalized_shuffled_reduced_numeric_dataset,\n",
    "    cleaned_normalized_shuffled_reduced_numeric_dataset,\n",
    "    n_clusters1=2,\n",
    "    n_clusters2=2,\n",
    "    selected_columns1=selected_columns,\n",
    "    selected_columns2=selected_columns\n",
    ")\n",
    "\n",
    "# Convert City column to integers in both datasets\n",
    "normalized_shuffled_reduced_numeric_dataset['City'] = normalized_shuffled_reduced_numeric_dataset['City'].astype(int)\n",
    "cleaned_normalized_shuffled_reduced_numeric_dataset['City'] = cleaned_normalized_shuffled_reduced_numeric_dataset['City'].astype(int)\n",
    "\n",
    "plot_columns = [col1, col2]  # Replace with the desired column names\n",
    "plot_custom_scatter(\n",
    "    normalized_shuffled_reduced_numeric_dataset,\n",
    "    cleaned_normalized_shuffled_reduced_numeric_dataset,\n",
    "    plot_columns=plot_columns,\n",
    "    city_mapping=city_mapping  # Pass the correct city mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_c\">c) Creation of Representative Training and Test Datasets and Different Similarity Measures </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will understand how representative training and test splits can be generated based on quantitative measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler (KL) Divergence\n",
    "\n",
    "\n",
    "The Kullback-Leibler (KL) divergence is a measure of how one probability distribution diverges from a reference distribution, quantifying the \"distance\" between them. It is commonly used to compare the similarity between two distributions, where a smaller KL divergence indicates closer alignment. KL divergence is especially useful in assessing the difference between training and test data distributions in machine learning. \n",
    "\n",
    "Below the KL divergence is defined for discrete probability distributions. Discrete distributions are probability distributions defined over a finite or countable set of outcomes.\n",
    "\n",
    "The KL divergence from a discrete distribution $P$ to a discrete distribution $Q$ is given by:\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- P(i) is the probability of an event i in the distribution P,\n",
    "- Q(i) is the probability of the same event i in the distribution Q.\n",
    "\n",
    "A KL divergence of 0 means the two distributions are identical, while higher values indicate greater divergence between them. \n",
    "\n",
    "The KL divergence is asymmetric, meaning $ D_{KL}(P || Q) \\neq D_{KL}(Q || P) $. This asymmetry indicates that the \"distance\" measured depends on the direction, as it penalizes differences between $ P $ and $ Q $ more heavily based on the reference distribution.\n",
    "\n",
    "\n",
    "### Kullback-Leibler Divergence for Gaussian Distributions\n",
    "\n",
    "When both distributions are Gaussian (bell-shaped curves), thereâ€™s a specific formula we can use to calculate this difference more easily. This formula only needs the mean and standard deviation of each Gaussian distribution, making it faster and simpler than the general approach for other types of distributions.\n",
    "\n",
    "#### KL Divergence for Gaussian Distributions\n",
    "\n",
    "For two Gaussian distributions with means and variances:\n",
    "\n",
    "$$\n",
    "P \\sim \\mathcal{N}(\\mu_P, \\sigma_P^2) \\quad \\text{and} \\quad Q \\sim \\mathcal{N}(\\mu_Q, \\sigma_Q^2)\n",
    "$$\n",
    "\n",
    "the KL divergence is given by:\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = \\ln \\frac{\\sigma_Q}{\\sigma_P} + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Task 1: Split Dataset into Train and Test\n",
    "def split_dataset(data, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input dataset.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    train (pd.DataFrame): Training dataset.\n",
    "    test (pd.DataFrame): Testing dataset.\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return train, test\n",
    "\n",
    "# Task 2: Create Histograms for Features\n",
    "def plot_histograms(train, test, features, num_bins=20):\n",
    "    \"\"\"\n",
    "    Creates histograms of the features for both train and test datasets, with one plot per feature.\n",
    "    \n",
    "    Parameters:\n",
    "    train (pd.DataFrame): Training dataset.\n",
    "    test (pd.DataFrame): Testing dataset.\n",
    "    features (list): List of feature names to plot.\n",
    "    num_bins (int): Number of bins to use for histogram comparison.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "\n",
    "        # Determine the range for bins based on both train and test data\n",
    "        min_value = min(train[feature].min(), test[feature].min())\n",
    "        max_value = max(train[feature].max(), test[feature].max())\n",
    "        bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Plot histograms with the shared bins for exact alignment\n",
    "        plt.hist(train[feature], bins=bins, alpha=0.5, label='Train', density=True, color='blue')\n",
    "        plt.hist(test[feature], bins=bins, alpha=0.5, label='Test', density=True, color='orange')\n",
    "        plt.title(f'Histogram of {feature}')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Task 3: Compute KL Divergence\n",
    "def compute_kl_divergence(train, test, features, num_bins=20):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler (KL) divergence for each feature to compare the train and test distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    train (pd.DataFrame): Training dataset.\n",
    "    test (pd.DataFrame): Testing dataset.\n",
    "    features (list): List of feature names to compare.\n",
    "    num_bins (int): Number of bins to use for histogram comparison.\n",
    "    \n",
    "    Returns:\n",
    "    kl_divergences (dict): Dictionary of KL divergence values for each feature.\n",
    "    \"\"\"\n",
    "    kl_divergences = {}\n",
    "    for feature in features:\n",
    "        # Create histograms for the train and test datasets for the feature\n",
    "        min_value = min(train[feature].min(), test[feature].min())\n",
    "        max_value = max(train[feature].max(), test[feature].max())\n",
    "        bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        train_hist, _ = np.histogram(train[feature], bins=bins, density=True)\n",
    "        test_hist, _ = np.histogram(test[feature], bins=bins, density=True)\n",
    "\n",
    "        # Ensure no zero values in histograms (for stable KL divergence calculation)\n",
    "        train_hist += 1e-10\n",
    "        test_hist += 1e-10\n",
    "\n",
    "        # Compute KL divergence\n",
    "        kl_div = entropy(train_hist, test_hist)\n",
    "        kl_divergences[feature] = kl_div\n",
    "        print(f\"KL Divergence for {feature}: {kl_div:.4f}\")\n",
    "\n",
    "    return kl_divergences\n",
    "\n",
    "def plot_histogram_with_gaussian(train, test, features, num_bins=20):\n",
    "    \"\"\"\n",
    "    Creates histograms of the features for both train and test datasets with an overlaid Gaussian distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    train (pd.DataFrame): Training dataset.\n",
    "    test (pd.DataFrame): Testing dataset.\n",
    "    features (list): List of feature names to plot.\n",
    "    num_bins (int): Number of bins to use for the histogram.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "\n",
    "        # Determine the range for bins based on both train and test data\n",
    "        min_value = min(train[feature].min(), test[feature].min())\n",
    "        max_value = max(train[feature].max(), test[feature].max())\n",
    "        bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Plot histograms with the shared bins for exact alignment\n",
    "        plt.hist(train[feature], bins=bins, alpha=0.5, label='Train', density=True, color='blue')\n",
    "        plt.hist(test[feature], bins=bins, alpha=0.5, label='Test', density=True, color='orange')\n",
    "\n",
    "        # Fit and plot Gaussian for train data\n",
    "        mean_train = train[feature].mean()\n",
    "        std_train = train[feature].std()\n",
    "        x_vals = np.linspace(min_value, max_value, 100)\n",
    "        plt.plot(x_vals, norm.pdf(x_vals, mean_train, std_train), color='blue', linestyle='--', label='Gaussian (Train)')\n",
    "\n",
    "        # Fit and plot Gaussian for test data\n",
    "        mean_test = test[feature].mean()\n",
    "        std_test = test[feature].std()\n",
    "        plt.plot(x_vals, norm.pdf(x_vals, mean_test, std_test), color='orange', linestyle='--', label='Gaussian (Test)')\n",
    "\n",
    "        plt.title(f'Histogram of {feature} with Gaussian Fit')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def compute_kl_divergence_gaussian(mean_p, std_p, mean_q, std_q):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two Gaussian distributions P and Q.\n",
    "    \n",
    "    Parameters:\n",
    "    mean_p (float): Mean of the first Gaussian distribution (P).\n",
    "    std_p (float): Standard deviation of the first Gaussian distribution (P).\n",
    "    mean_q (float): Mean of the second Gaussian distribution (Q).\n",
    "    std_q (float): Standard deviation of the second Gaussian distribution (Q).\n",
    "    \n",
    "    Returns:\n",
    "    float: KL divergence between the two Gaussian distributions.\n",
    "    \"\"\"\n",
    "    kl_divergence = np.log(std_q / std_p) + (std_p**2 + (mean_p - mean_q)**2) / (2 * std_q**2) - 0.5\n",
    "    return kl_divergence\n",
    "\n",
    "def compute_kl_divergence_gaussian_multiple(train, test, features):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between Gaussian approximations of multiple features in train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    features (list): List of feature names to compute KL divergence for.\n",
    "    train (pd.DataFrame): Training dataset.\n",
    "    test (pd.DataFrame): Testing dataset.\n",
    "    \n",
    "    Returns:\n",
    "    kl_divergences (dict): Dictionary of KL divergence values for each feature.\n",
    "    \"\"\"\n",
    "    kl_divergences = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        # Calculate mean and standard deviation for the feature in both train and test sets\n",
    "        mean_train = train[feature].mean()\n",
    "        std_train = train[feature].std()\n",
    "        mean_test = test[feature].mean()\n",
    "        std_test = test[feature].std()\n",
    "        \n",
    "        # Compute KL divergence for the current feature\n",
    "        kl_divergence = compute_kl_divergence_gaussian(mean_train, std_train, mean_test, std_test)\n",
    "        kl_divergences[feature] = kl_divergence\n",
    "        print(f\"Gaussian KL Divergence for {feature}: {kl_divergence:.4f}\")\n",
    "    \n",
    "    return kl_divergences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can change the variables test_size and random_state. \\\n",
    "A different test_size changes the sizes of the training and test splits. \\\n",
    "A different random_state decides different samples for the training and test splits.\n",
    "\n",
    "When answering the moodle questions use 'test_size = 0.5' and 'random_state = 42'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = None    # CHANGE test_size TO ANY Float BETWEEN 0.0 AND 1.0 \n",
    "                    # (e.g. 0.3 equals 30% of data is used for the test set, the rest for the train set)\n",
    "random_state = None # CHANGE random_state TO ANY Integer FOR NEW RANDOM SPLIT\n",
    "\n",
    "## DO NOT CHANGE THE CODE BELOW\n",
    "check_value(test_size, \"test_size\")\n",
    "check_value(random_state, \"random_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can change the threshold for the z-score outlier detection for the remaining computations in chapter c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = None  # CHANGE threshold from None to any positive number, default value is usually 3.0\n",
    "\n",
    "## DO NOT CHANGE THE CODE BELOW\n",
    "check_value(threshold, \"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codeblock computes and plots the distributions for all cities of chosen columns (col1, col2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "numeric_train, numeric_test = split_dataset(allcities_numeric_dataset, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Plot histograms and gaussian distributions of the features\n",
    "features = [col1, col2]\n",
    "plot_histogram_with_gaussian(numeric_train, numeric_test, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the KL Divergence is computed - for the discrete and gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Compute KL Divergence between train and test distributions for each feature\n",
    "kl_divergences = compute_kl_divergence(numeric_train, numeric_test, features)\n",
    "kl_divergences_gaussian = compute_kl_divergence_gaussian_multiple(numeric_train, numeric_test, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
