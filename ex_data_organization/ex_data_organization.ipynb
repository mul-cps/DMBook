{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a0d7a0",
   "metadata": {},
   "source": [
    "# Exercise - Data Organization & Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b05e025-9843-410e-80cd-f2db24148da8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This is an exercise on the basics of data organization techniques, focussing on data splitting, and validation strategies in machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62db42",
   "metadata": {},
   "source": [
    "## <a id=\"sec_toc\">Content</a>\n",
    "\n",
    "[Learning Objectives](#sec_0)\n",
    "\n",
    "[a) Prepare and analyse the data](#sec_a)\n",
    "\n",
    "[b) Simple Train Test Split](#sec_b)\n",
    "\n",
    "[c) Implementing K-Fold Cross-Validation](#sec_c)\n",
    "\n",
    "[d) Stratified K-Fold Cross Validation](#sec_d)\n",
    "\n",
    "[Bonus) Plot the data](#sec_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda28b9c",
   "metadata": {},
   "source": [
    "### <a id=\"sec_0\">Learning Objectives</a>\n",
    "\n",
    "* Identify and apply necessary pre-processing steps to prepare data for analysis. \n",
    "* Become familiar with evaluation metrics, such as Accuracy, Precision, Recall and F1-score.\n",
    "* Understand and Interpret these evaluation metrics.\n",
    "* Understand the basics of train-test splits and k-fold cross validation.\n",
    "* Determine when and why to use these splitting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f27a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:20:17.851829800Z",
     "start_time": "2024-11-11T06:20:17.769794500Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208862e-7b8b-435a-8ef5-4bbf09fe9feb",
   "metadata": {},
   "source": [
    "### Import the needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbdd51-4cd9-4dd3-a27e-3809b3705f60",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.851829800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from typing import Union, Dict, Literal, Sequence\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059312e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.853466400Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 1\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e5901",
   "metadata": {},
   "source": [
    "### Define all functions that are needed in the notebook\n",
    "The code cell below defines all the functions used in the notebook. \\\n",
    "If you want to save space you can click on the blue tab on the left to truncate the code cell.\\\n",
    "This is only possible, if you are using jupyterhub.\\\n",
    "This improves the readability of the Python code and should help you to understand the Python code. \\\n",
    "If you are interested in the code, expand the code cell by clicking on the blue tab on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22daf3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.855804900Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "def binarize_labels(labels: pd.DataFrame, mapping: Dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\"Maps the label values to int-values, based on the mapping dictionary.\"\"\"\n",
    "    c_labels = labels.copy()\n",
    "    c_labels[c_labels.columns[0]] = c_labels[c_labels.columns[0]].map(mapping)\n",
    "    return c_labels\n",
    "\n",
    "\n",
    "def binarize_labels_rev(labels: pd.DataFrame, mapping: Dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\"Maps the encoded label values back to their original string values.\"\"\"\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}  # Create reverse mapping\n",
    "    c_labels = labels.copy()\n",
    "    c_labels[c_labels.columns[0]] = c_labels[c_labels.columns[0]].map(reverse_mapping)\n",
    "    return c_labels\n",
    "\n",
    "\n",
    "def check_data_consistency(data, normalized_data, labels):\n",
    "    if normalized_data.shape != data.shape:\n",
    "        print(\n",
    "            \"Deleted columns (because of NaN-values): \",\n",
    "            set(data.columns).symmetric_difference(set(normalized_data.columns)),\n",
    "        )\n",
    "    else:\n",
    "        print(\"No columns contain any NaN values.\")\n",
    "\n",
    "    if normalized_data.isna().sum().sum() != 0:\n",
    "        raise Exception(\"Error: normalized_data still contains NaN values.\")\n",
    "\n",
    "    # check if data and labels have the same number of rows\n",
    "    if normalized_data.shape[0] != labels.shape[0]:\n",
    "        raise Exception(\"Error: data and labels do not have the same number of rows.\")\n",
    "    \n",
    "\n",
    "def check_dims(input_data: pd.DataFrame, labels: pd.DataFrame) -> None:\n",
    "    \"\"\"Takes two DataFrames and prints their shape.\"\"\"\n",
    "    print(\"dimensions of the data: \", input_data.shape, \"type: \", type(input_data))\n",
    "    print(\"dimension of the labels: \", labels.shape, \"type: \", type(labels))\n",
    "\n",
    "\n",
    "def check_factor(stratification_factor):\n",
    "    if stratification_factor is None:\n",
    "        print(\"stratification_factor is still None. Please enter a valid value for stratification_factor.\")\n",
    "        raise Exception(\"Execution stopped because stratification_factor is not defined.\")\n",
    "    \n",
    "    if stratification_factor<0 or stratification_factor>=1:\n",
    "        print(\"The entered value fotr the stratification_factor is invalid. Please enter a valid value for stratification_factor.\")\n",
    "        raise Exception(\"Execution stopped because stratification_factor is not valid.\")\n",
    "    \n",
    "\n",
    "def check_k(k):\n",
    "    if k is None:\n",
    "        print(\"k is still None. Please enter a valid value for k.\")\n",
    "        raise Exception(\"Execution stopped because k is not defined.\")\n",
    "    \n",
    "    if k<3 or k>10:\n",
    "        print(\"The value for k is invalid. Please enter a valid value for k.\")\n",
    "        raise Exception(\"Execution stopped because the entered value for k is not valid.\")\n",
    "\n",
    "\n",
    "def check_plot_values(X, y):\n",
    "    if X is None:\n",
    "        print(\"X_plot is still None. Please enter a valid value for X_plot.\")\n",
    "        raise Exception(\"Execution stopped because X_plot is not defined.\")\n",
    "\n",
    "    if y is None:\n",
    "        print(\"y_plot is still None. Please enter a valid value for y_plot.\")\n",
    "        raise Exception(\"Execution stopped because y_plot is not defined.\")\n",
    "    \n",
    "\n",
    "def check_values(city1, city2):\n",
    "    cities = [\"Chicago\",\"New York City\",\"Dallas\",\"Phoenix\",\"Philadelphia\",\"Los Angeles\",\"San Diego\",\"San Jose\",\"Houston\", \"San Antonio\"]\n",
    "    if city1 is None:\n",
    "        print(\"city1 is still None. Please enter a valid city name.\")\n",
    "        raise Exception(\"Execution stopped because city1 is not defined.\")\n",
    "\n",
    "    if city2 is None:\n",
    "        print(\"city2 is still None. Please enter a valid city name.\")\n",
    "        raise Exception(\"Execution stopped because city2 is not defined.\")\n",
    "    \n",
    "    if city1 not in cities:\n",
    "        print(\"city1 is an invalid value. Please enter a valid city name. Valid cities are listed in the output of the cell above.\")\n",
    "        raise Exception(\"Execution stopped because city1 is not valid.\")\n",
    "\n",
    "    if city2 not in cities:\n",
    "        print(\"city2 is an invalid value. Please enter a valid city name. Valid cities are listed in the output of the cell above.\")\n",
    "        raise Exception(\"Execution stopped because city2 is not valid.\")\n",
    "    \n",
    "    return [city1, city2]\n",
    "\n",
    "\n",
    "def check_value(random_seed):\n",
    "    if random_seed is False:\n",
    "        print(\"random_seed is still False. Please enter a valid value (e.g. True).\")\n",
    "        raise Exception(\"Execution stopped because random_seed is still False.\")\n",
    "    \n",
    "    \n",
    "def create_city_mapping(cities: list[str]) -> Dict[str, int]:\n",
    "    \"\"\"Takes a list of values, in this case cities and returns a dictionary,\n",
    "    where each of these cities is assigned to an integer value.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for i, city in enumerate(cities):\n",
    "        mapping.update({city: i})\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def create_k_folds(\n",
    "    data: pd.DataFrame,\n",
    "    labels: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    fold: Union[KFold, StratifiedKFold] = KFold,\n",
    ") -> tuple[pd.DataFrame]:\n",
    "    \"\"\"Divides a dataset into k different equivalent subsets and returns them.\"\"\"\n",
    "    data_shuffled, labels_shuffled = shuffle(data, labels, random_state=42)\n",
    "    kf = fold(n_splits=k)\n",
    "    folds_data = []\n",
    "    folds_labels = []\n",
    "    for _, val_index in kf.split(data_shuffled, labels_shuffled):\n",
    "        fold_d = data_shuffled.iloc[val_index]\n",
    "        fold_l = labels_shuffled.iloc[val_index]\n",
    "        folds_data.append(fold_d)\n",
    "        folds_labels.append(fold_l)\n",
    "    return folds_data, folds_labels\n",
    "\n",
    "\n",
    "def create_legend(mapping: Dict[str, int], y_test: pd.DataFrame) -> None:\n",
    "    \"\"\"Creates a legend for the scatter plot, based on the 'mapping' of the label values.\"\"\"\n",
    "    city_labels = {v: k for k, v in mapping.items()}\n",
    "    k = len(mapping) - 1\n",
    "    handles = [\n",
    "        plt.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"k\",\n",
    "            markerfacecolor=plt.cm.Paired(i / k),\n",
    "            markersize=10,\n",
    "            label=city_labels[i],\n",
    "        )\n",
    "        for i in np.unique(y_test.values)\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=handles, title=\"Cities\", bbox_to_anchor=(1.50, 1), loc=\"upper right\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_cross_val(\n",
    "    kf_data: list[pd.DataFrame], kf_labels: list[pd.DataFrame]\n",
    ") -> tuple[float]:\n",
    "    \"\"\"Performs the model training and returns the different evaluation metrics.\"\"\"\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    F1_scores = []\n",
    "    training_accuracies = []\n",
    "    for i in range(len(kf_data)):\n",
    "        kf_data_copy = kf_data.copy()\n",
    "        kf_labels_copy = kf_labels.copy()\n",
    "        X_test = kf_data_copy.pop(i)\n",
    "        y_test = kf_labels_copy.pop(i).to_numpy().squeeze()\n",
    "        X_train = pd.concat(kf_data_copy, ignore_index=True)\n",
    "        y_train = pd.concat(kf_labels_copy, ignore_index=True).to_numpy().ravel()\n",
    "\n",
    "        model = linear_model.LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        training_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        F1_scores.append(f1_score(y_test, y_pred))\n",
    "        training_accuracies.append(float(training_acc))\n",
    "    return (\n",
    "        np.array(accuracies),\n",
    "        np.array(precisions),\n",
    "        np.array(recalls),\n",
    "        np.array(F1_scores),\n",
    "        np.array(training_accuracies),\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_label_values(\n",
    "    data: pd.DataFrame, labels: pd.DataFrame, values: list[str]\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Filters the data based on the values in the 'labels' DataFrame.\n",
    "    After applying this function, the dataset only contains rows with\n",
    "    the label described in the 'values' list.\n",
    "    \"\"\"\n",
    "    indices_to_keep = labels[labels[labels.columns[0]].isin(values)].index\n",
    "    filtered_data = data.loc[indices_to_keep]\n",
    "    filtered_labels = labels.loc[indices_to_keep]\n",
    "    return filtered_data, filtered_labels\n",
    "\n",
    "\n",
    "def logistic_reg_3d(X_train_3dims, y_plot, xx, yy):\n",
    "    logistic_reg_plot = linear_model.LogisticRegression()\n",
    "    model = logistic_reg_plot.fit(X_train_3dims, y_plot.to_numpy().ravel())\n",
    "    coef = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    zz = -(coef[0] * xx + coef[1] * yy + intercept) / coef[2]\n",
    "    return zz, model\n",
    "\n",
    "\n",
    "def get_principal_components(data: pd.DataFrame, nr_cols: int) -> pd.DataFrame:\n",
    "    \"\"\"Takes a DataFrame and returns the reduced dataset based on principal component analysis.\"\"\"\n",
    "    U, s, VT = np.linalg.svd(data, full_matrices=False)\n",
    "    Vk = VT[:, :nr_cols]\n",
    "    P = np.dot(data, Vk)\n",
    "    cols = [f\"{i+1}. Principal Component\" for i in range(nr_cols)]\n",
    "    df = pd.DataFrame(P, columns=cols)\n",
    "    print(\"variance kept [%]: \", (s**2 / sum(s**2)) * 100)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_results(data: pd.DataFrame, labels: pd.DataFrame, k_range: list[int], fold: Union[KFold, StratifiedKFold] = KFold) -> tuple[list[float]]:\n",
    "    k_result_train = []\n",
    "    k_result_test = []\n",
    "    k_result_f1 = []\n",
    "    for k in k_range:\n",
    "        data_folds, label_folds = create_k_folds(data, labels, k, fold)\n",
    "        acc_test, _, _, f1, acc_train = eval_cross_val(data_folds, label_folds)\n",
    "        mean_acc_test = acc_test.mean()\n",
    "        mean_acc_train = acc_train.mean()\n",
    "        mean_f1_test = f1.mean()\n",
    "        k_result_test.append(mean_acc_test)\n",
    "        k_result_train.append(mean_acc_train)\n",
    "        k_result_f1.append(mean_f1_test)\n",
    "    return k_result_train, k_result_test, k_result_f1\n",
    "\n",
    "\n",
    "def get_ymin_ymax_scores(scores: Union[Sequence[np.ndarray], np.ndarray]) -> tuple[float, float]:\n",
    "    selected_columns_val = np.concatenate([score[0] for score in scores])\n",
    "    selected_columns_test = np.concatenate([score[0] for score in scores])\n",
    "    return get_ymin_ymax(selected_columns_val, selected_columns_test)\n",
    "\n",
    "\n",
    "def get_ymin_ymax(acc_train, acc_test) -> tuple[float, float]:\n",
    "    cols = np.concatenate([acc_train, acc_test])\n",
    "    ymin = np.min(cols)-0.01\n",
    "    ymax = np.max(cols)+0.01\n",
    "    return ymin, ymax\n",
    "\n",
    "\n",
    "def minorize_label(\n",
    "    data: pd.DataFrame, labels: pd.DataFrame, value: str, frac: float = 0.5\n",
    ") -> tuple[pd.DataFrame]:\n",
    "    \"\"\"Randomly deletes 'frac'*100 percent of the data, where the label equals 'value'.\"\"\"\n",
    "    data_shuffled, labels_shuffled = shuffle(data, labels, random_state=42)\n",
    "    label_indices = labels_shuffled[\n",
    "        labels_shuffled[labels_shuffled.columns[0]] == value\n",
    "    ].index\n",
    "    num_rows_to_delete = int(len(label_indices) * frac)\n",
    "    rows_to_delete = np.random.choice(label_indices, num_rows_to_delete, replace=False)\n",
    "    data_min = data_shuffled.drop(rows_to_delete)\n",
    "    labels_min = labels_shuffled.drop(rows_to_delete)\n",
    "    return data_min, labels_min\n",
    "\n",
    "\n",
    "def plot_acc_per_fold(\n",
    "    scores: tuple[float], y_lim_min: float, y_lim_max: float, text: str = \"Training and Validation Accuracy for each fold\"\n",
    ") -> None:\n",
    "    \"\"\"Plots the training and validation accuracy of the model for each fold.\"\"\"\n",
    "    val_accuracies, train_accuracies = scores[0], scores[4]\n",
    "    k = len(val_accuracies)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6.5))\n",
    "    ax.plot(range(1, k + 1), train_accuracies, label=\"Training Accuracy\", marker=\"o\")\n",
    "    ax.plot(range(1, k + 1), val_accuracies, label=\"Validation Accuracy\", marker=\"o\")\n",
    "    ax.set_ylim(y_lim_min, y_lim_max)\n",
    "    ax.set_xlabel(\"Fold Number\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.title.set_text(text)\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_3dim(\n",
    "    data: pd.DataFrame,\n",
    "    labels: pd.DataFrame,\n",
    "    title: str,\n",
    "    mode: Literal[\"orig axes\", \"pca\", \"tsne\"] = \"pca\",\n",
    "    column_names: list[str] = [\"Severity_Score\", \"Health_Risk_Score\", \"severerisk\"],\n",
    ") -> None:\n",
    "    \"\"\"Performs PCA, TSNE or no dimensionality reduction on the data and plots it in a 3-dimensional plot.\"\"\"\n",
    "    if mode == \"pca\":\n",
    "        data_3d = get_principal_components(data, 3)\n",
    "    elif mode == \"tsne\":\n",
    "        data_3d = TSNE(n_components=3).fit_transform(data)\n",
    "        data_3d = pd.DataFrame(data_3d, columns=[f\"{i}. dim tsne\" for i in range(1, 4)])\n",
    "    elif mode == \"orig axes\":\n",
    "        data_3d = data[column_names].copy()\n",
    "\n",
    "    first_col, second_col, third_col = (\n",
    "        data_3d.iloc[:, 0].to_frame(),\n",
    "        data_3d.iloc[:, 1].to_frame(),\n",
    "        data_3d.iloc[:, 2].to_frame(),\n",
    "    )\n",
    "\n",
    "    cities = labels[labels.columns[0]].unique()\n",
    "\n",
    "    mapping_all_cities = create_city_mapping(cities)\n",
    "    all_labels_bin = binarize_labels(labels, mapping_all_cities)\n",
    "\n",
    "    # Creating a 3D scatter plot\n",
    "    fig = plt.figure(figsize=(10.5, 6.5))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(\n",
    "        first_col,\n",
    "        second_col,\n",
    "        third_col,\n",
    "        c=all_labels_bin,\n",
    "        edgecolors=\"k\",\n",
    "        marker=\"o\",\n",
    "        cmap=plt.cm.Paired,\n",
    "    )\n",
    "    ax.set_xlabel(first_col.columns[0])\n",
    "    ax.set_ylabel(second_col.columns[0])\n",
    "    ax.set_zlabel(third_col.columns[0])\n",
    "    ax.title.set_text(title)\n",
    "    create_legend(mapping_all_cities, all_labels_bin)\n",
    "\n",
    "\n",
    "def plot_decision_boundary_2d(model, xx, yy, first_col, second_col, y_plot, mapping):\n",
    "    # calculate decision boundary by inserting meshgrid values\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # plot the decision boundary and the datapoints\n",
    "    title = \"Logistic Regression Decision Boundary and test data points\"\n",
    "    scatter_plot(\n",
    "        first_col,\n",
    "        second_col,\n",
    "        y_plot,\n",
    "        title,\n",
    "        contour=True,\n",
    "        spec=[xx, yy, Z, 0.4, plt.cm.Paired],\n",
    "    )\n",
    "    create_legend(mapping, y_plot)\n",
    "\n",
    "\n",
    "def plot_decision_boundary_3d(model, xx, yy, zz, y_plot, first_col, second_col, third_col, mapping):\n",
    "    # Calculate decision boundary using meshgrid values\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel(), zz.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Create a 3D plot with the decision boundary and data points\n",
    "    fig = plt.figure(figsize=(10.5, 6.5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    title = \"Logistic Regression 3D Decision Boundary and Test Data Points\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    y_plot_array = y_plot.to_numpy().ravel()\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(first_col, second_col, third_col, c= y_plot_array, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    # Plot the decision boundary in 3D\n",
    "    ax.plot_surface(xx, yy, zz, color='lightblue', alpha=0.5)\n",
    "\n",
    "    # Label axes\n",
    "    ax.set_xlabel(\"1. Principal Component\")\n",
    "    ax.set_ylabel(\"2. Principal Component\")\n",
    "    ax.set_zlabel(\"3. Principal Component\")\n",
    "\n",
    "    # Add legend\n",
    "    create_legend(mapping, y_plot)\n",
    "\n",
    "\n",
    "def prepare_data(X_plot):\n",
    "    X_train_3dims = get_principal_components(X_plot, 3)\n",
    "    first_col = X_train_3dims.iloc[:, 0].to_frame()\n",
    "    second_col = X_train_3dims.iloc[:, 1].to_frame()\n",
    "    third_col = X_train_3dims.iloc[:,2].to_frame()\n",
    "    x_min, x_max = first_col.min().values[0] - 1, first_col.max().values[0] + 1\n",
    "    y_min, y_max = second_col.min().values[0] - 1, second_col.max().values[0] + 1\n",
    "    z_min, z_max = third_col.min().values[0] - 1, third_col.max().values[0] + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) \n",
    "    return X_train_3dims, xx, yy, first_col, second_col, third_col\n",
    "\n",
    "    \n",
    "def prepare_4_plot(X_train: pd.DataFrame) -> tuple[Union[pd.DataFrame, npt.NDArray]]:\n",
    "    \"\"\"Returns the first two columns of 'X_train' and points within the plot spectrum as a meshgrid with a\n",
    "    distance of 0.1 between the different points. This function is used as preprocessing for the plot of\n",
    "    the decision areas of the logistic regression.\n",
    "    \"\"\"\n",
    "    first_col = X_train.iloc[:, 0].to_frame()\n",
    "    second_col = X_train.iloc[:, 1].to_frame()\n",
    "    x_min, x_max = first_col.min().values[0] - 1, first_col.max().values[0] + 1\n",
    "    y_min, y_max = second_col.min().values[0] - 1, second_col.max().values[0] + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    return first_col, second_col, xx, yy\n",
    "\n",
    "\n",
    "def print_cross_val_result(scores: list[float]) -> None:\n",
    "    \"\"\"Takes the list of scores, that is returned by the function\n",
    "    `eval_cross_val` and prints the results.\n",
    "    \"\"\"\n",
    "    score_names = [\n",
    "        \"accuracies\",\n",
    "        \"precisions\",\n",
    "        \"recalls\",\n",
    "        \"F1_scores\",\n",
    "        \"training accuracies\",\n",
    "    ]\n",
    "    for score, name in zip(scores, score_names):\n",
    "        print_score(name, score)\n",
    "\n",
    "\n",
    "def print_score(metric: str, score: npt.NDArray) -> None:\n",
    "    \"\"\"Takes the 'metric' and the 'score' of the cross validation and prints\n",
    "    the values, the mean and the standard deviation.\n",
    "    \"\"\"\n",
    "    print(f\"\\033[4m{metric}\\033[0m\")\n",
    "    print(f\"\\033[1mCross validation scores:\\033[0m \", score)\n",
    "    print(\n",
    "        f\"\\033[1mMean\\033[0m of the cross validation scores: \", round(score.mean(), 4)\n",
    "    )\n",
    "    print(\n",
    "        f\"\\033[1mStandard deviation\\033[0m of the cross validation scores: \",\n",
    "        round(score.std(), 4),\n",
    "        \"\\n\",\n",
    "    )\n",
    "\n",
    "\n",
    "def scatter_plot(\n",
    "    first_col: pd.DataFrame,\n",
    "    second_col: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    title: str,\n",
    "    contour: bool = False,\n",
    "    spec: list = None,\n",
    ") -> None:\n",
    "    \"\"\"Creates a scatter plot of the data points. This function also takes two optional parameters, ``contour`` and ``spec``\n",
    "    these two inputs enable that the scatter plot also contains the decision boundary of the linear model.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.6, 0.75])\n",
    "    if contour:\n",
    "        ax.contourf(spec[0], spec[1], spec[2], alpha=spec[3], cmap=spec[4])\n",
    "    ax.scatter(\n",
    "        first_col.values,\n",
    "        second_col.values,\n",
    "        c=y_test.values,\n",
    "        edgecolors=\"k\",\n",
    "        marker=\"o\",\n",
    "        cmap=plt.cm.Paired,\n",
    "    )\n",
    "    ax.title.set_text(title)\n",
    "    ax.set_xlabel(f\"{first_col.columns[0]}\")\n",
    "    ax.set_ylabel(f\"{second_col.columns[0]}\")\n",
    "\n",
    "\n",
    "def scatter_plot_train_test(filtered_data, X_train, X_test):\n",
    "    \"\"\"creates a scatter plot of the train and test data. \n",
    "    This is done to visualize, that the test data is randomly drawn from the test data.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 4.5))\n",
    "    U, s, VT = np.linalg.svd(filtered_data, full_matrices=False)\n",
    "    Vk = VT[:, :2]\n",
    "    X_2dims_train = np.dot(X_train, Vk)\n",
    "    X_2dims_test = np.dot(X_test, Vk)\n",
    "\n",
    "    X_2dims_train_df = pd.DataFrame(X_2dims_train, columns=[\"1. Principal Component\", \"2. Principal Component\"])\n",
    "    X_2dims_train_df['Set'] = 'Train'\n",
    "\n",
    "    X_2dims_test_df = pd.DataFrame(X_2dims_test, columns=[\"1. Principal Component\", \"2. Principal Component\"])\n",
    "    X_2dims_test_df['Set'] = 'Test'\n",
    "\n",
    "    X_2dims_combined_df = pd.concat([X_2dims_train_df, X_2dims_test_df], ignore_index=True)\n",
    "\n",
    "    # Plot the combined data with a single scatter plot\n",
    "    sns.scatterplot(\n",
    "        data=X_2dims_combined_df,\n",
    "        x=\"1. Principal Component\",\n",
    "        y=\"2. Principal Component\",\n",
    "        hue=\"Set\",\n",
    "        alpha=0.8,\n",
    "        legend = False\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_plot_folds(\n",
    "    k_range: list[int],\n",
    "    k_results_train: list[float],\n",
    "    k_result_test: list[float],\n",
    "    k_result_f1: list[float], \n",
    "    y_lim_min: float, \n",
    "    y_lim_max: float,\n",
    "    mode: Literal[\"Accuracy\", \"F1-Score\"] = \"Accuracy\",\n",
    "    text: str = \"\",\n",
    ") -> None:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6.5))\n",
    "    if mode == \"Accuracy\":\n",
    "        ax.plot(\n",
    "            k_range, k_result_test, label=\"Mean Test Accuracy\", marker=\"o\", color=\"b\"\n",
    "        )\n",
    "        ax.plot(\n",
    "            k_range, k_results_train, label=\"Mean Train Accuracy\", marker=\"o\", color=\"r\"\n",
    "        )\n",
    "    else:\n",
    "        ax.plot(k_range, k_result_f1, label=\"Mean Test F1-Score\", marker=\"o\", color=\"b\")\n",
    "\n",
    "    ax.set_xlabel(\"Number of Folds (k)\")\n",
    "    ax.set_ylabel(f\"Mean {mode}\")\n",
    "    ax.set_ylim(y_lim_min, y_lim_max)\n",
    "    ax.title.set_text(f\"Mean {mode} vs. Number of Folds \\n {text}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_test_split(\n",
    "    X: pd.DataFrame, y: pd.DataFrame, test_size: float = 0.2, seed: bool = True\n",
    ") -> tuple[pd.DataFrame]:\n",
    "    \"\"\"This function is splitting the input data and the corresponding labels into random train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    X: The input data\n",
    "    y: The labels for the input data\n",
    "    test_size (float): The size ratio of the test set compared to all data points (0 <= test_size <= 1, default value: 0.2).\n",
    "\n",
    "    Returns:\n",
    "    Returns a tuple with the following values:\n",
    "    X_train: The input data, that is used for training.\n",
    "    X_test: The input data, that is used for testing.\n",
    "    y_train: The labels, that are used for training.\n",
    "    y_test: The labels, that are used for testing.\n",
    "    \"\"\"\n",
    "    np.random.seed(3) if seed else None\n",
    "    arr_rand = np.random.rand(X.shape[0])\n",
    "    split = arr_rand < np.percentile(arr_rand, 100 - test_size * 100)\n",
    "\n",
    "    X_train = X[split]\n",
    "    X_test = X[~split]\n",
    "    y_train = y[split]\n",
    "    y_test = y[~split]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ce2d0",
   "metadata": {},
   "source": [
    "## <a id=\"sec_a\">a) Prepare and analyse the data</a>\n",
    "The dataset is stored in a csv table and consists of columns representing, for example, different characteristics in the data. \\\n",
    "The ``cities`` column contains the label for each row of data. \\\n",
    "First, the .csv table is read into a DataFrame (DataFrame: an efficient and powerful data storage type in Python). \\\n",
    "The next step is to split the data columnwise into the input data columns and the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70079cf",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450ec0d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.858100300Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# get the filepath and import data\n",
    "orig_data = pd.read_csv(\"urban_air_quality_and_health.csv\")\n",
    "\n",
    "# display 5 rows of the dataset\n",
    "orig_data.iloc[[5, 120, 250, 650, 907]].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883b381",
   "metadata": {},
   "source": [
    "### Split dataset in data and labels\n",
    "The next two sections define the columns to be removed and print them. \\\n",
    "Later, the dataset will be split into labels and data, with only the columns needed for the experiment. \n",
    "\n",
    "<u>TASK:</u> Inspect the columns that have been removed (i.e. ``columns_to_drop``). \\\n",
    "What do the removed columns have in common? \\\n",
    "Use this table to solve the first question of the moodle test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe25c97",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.860376500Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# prints the columns, that should be removed\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"datetimeEpoch\", \"datetime\", \"Month\", \"preciptype\", \"sunrise\", \"sunriseEpoch\", \"sunset\",\n",
    "    \"sunsetEpoch\", \"conditions\", \"description\", \"icon\", \"stations\", \"source\", \"Season\",\n",
    "    \"Day_of_Week\", \"feelslikemin\", \"feelslikemax\", \"feelslike\", \"Condition_Code\", \"Is_Weekend\",\n",
    "]\n",
    "\n",
    "extra_drop = [\n",
    "    \"tempmax\", \"tempmin\", \"temp\", \"dew\", \"humidity\", \"precip\", \"precipprob\", \"precipcover\",\n",
    "    \"snow\", \"snowdepth\", \"windgust\", \"windspeed\", \"winddir\", \"pressure\", \"cloudcover\", \n",
    "    \"visibility\", \"solarradiation\", \"solarenergy\", \"uvindex\", \"moonphase\", \"City\",\n",
    "]\n",
    "\n",
    "print(\"Columns to be removed from the dataset\")\n",
    "orig_data[columns_to_drop].iloc[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed688122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:20:17.988409Z",
     "start_time": "2024-11-11T06:20:17.862706800Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# save the city column as labels\n",
    "labels = orig_data[\"City\"].to_frame()\n",
    "\n",
    "# preprocess data, remove some data columns\n",
    "data = orig_data.drop(columns=columns_to_drop)\n",
    "data = data.drop(columns=extra_drop)\n",
    "\n",
    "print(\"This are some rows of the dataset after these preprocessing steps: \")\n",
    "data.iloc[[5, 120, 250, 650, 907]].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd94d4",
   "metadata": {},
   "source": [
    "The dataset currently consists of 5 columns that describe different features in the data.\n",
    "\n",
    "The following code cell displays the dimension of the original data in the form ``(rows, columns)``. \\\n",
    "Below, all remaining column names are printed to give an overview. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb94188",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.864916500Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# compare the dimensions of the data and the labels\n",
    "print(\"dimensions of the data before preprocessing: \", orig_data.shape)\n",
    "print(\"dimension of the data after preprocessing: \", data.shape)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40bcc5b",
   "metadata": {},
   "source": [
    "### Normalize and clean data \n",
    "The code in the next section normalises the data and removes columns containing nan values. \\\n",
    "NaN means 'not a number', these values cannot be processed, so it is important to remove these values before using the data.\n",
    "\n",
    "\n",
    "\n",
    "**Rule of thumb**\n",
    "\n",
    "If the data dimensions are the same, e.g. all characteristics have the unit m/s, it is not necessary to divide by the standard deviation. \\\n",
    "Normalise by making the data mean free:\n",
    "$$\n",
    "  \\text{normalized feature} = {x - \\mu}\n",
    "$$\n",
    "\n",
    "In this dataset, the features have different dimensions, so it is useful to normalise the \\\n",
    "data by making it mean free with unit variance (Z-Score). \n",
    "$$\n",
    "  \\text{normalized feature} = \\frac{{x - \\mu}}{{\\sigma}}\n",
    "$$\n",
    "If you are training machine learning models with enough data, this rule does not necessarily apply, but it is a good reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7f29d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.866012500Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# normalize the data\n",
    "normalized_data = (data - data.mean()) / data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211175a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.868414500Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# remove columns, containing nan values\n",
    "normalized_data = normalized_data.dropna(axis=1)\n",
    "\n",
    "check_data_consistency(data, normalized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9b33a",
   "metadata": {},
   "source": [
    "### Preparation of the data\n",
    "The task is to train a logistic regression model that predicts the city where the air quality data are collected. \\\n",
    "We are going to train a binary logistic regression model, so we need to prepare the data so that it only contains data from two cities.\\\n",
    "It is important to note that the cities are represented with different frequencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60501cf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.870615600Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "frequency_counts = labels[labels.columns[0]].value_counts().reset_index()\n",
    "frequency_counts.columns = [\"City\", \"Frequency\"]\n",
    "frequency_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45196781",
   "metadata": {},
   "source": [
    "From all the cities in the data, two are selected to train the model on. \\\n",
    "The dataset now consists of all the rows containing the labels of the different cities. \\\n",
    "The model will be trained on this data, so the model will only decide between these two cities. \\\n",
    "The labels DataFrame currently contains city names. \\\n",
    "To enable the model to classify on the different cities, the city names are changed to a binary coding. \\\n",
    "All entries that were, for example, New York City are now 0 and Los Angeles is now 1.\n",
    "\n",
    "<u>TASK:</u> Please choose two different cities of the table above and replace the None values below with the city names. \\\n",
    "The city names have to be in apostrophs, which are indicating the datatype as a string (e.g. \"Phoenix\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "city1 = None # please change None to a city, this is listed above \n",
    "city2 = None # please change None to a city, this is listed above \n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS CODE\n",
    "values = check_values(city1, city2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df682ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.876198100Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# filters data, only rows with the cities above are left\n",
    "filtered_data, filtered_labels = filter_label_values(normalized_data, labels, values)\n",
    "\n",
    "# changes all vlaues in label, that it only contains numbers\n",
    "mapping = create_city_mapping(values)\n",
    "print(\"\\n mapping: \", mapping)\n",
    "binarized_labels = binarize_labels(filtered_labels, mapping)\n",
    "\n",
    "if filtered_data.shape[0] != binarized_labels.shape[0]:\n",
    "    raise Exception(\"Error: data and labels do not have the same number of rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff9471",
   "metadata": {},
   "source": [
    "Data preparation is now complete. \\\n",
    "With all this pre-processing done, let's print the labels and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396790d6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.878382500Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "print(\"labels:\")\n",
    "binarized_labels = binarized_labels.reset_index()['City'].to_frame()\n",
    "binarized_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a633b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.880480100Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "print(\"data:\")\n",
    "filtered_data = filtered_data.reset_index().drop([\"index\"], axis=1)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b753e8",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97999c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a id=\"sec_b\">b) Simple Train-Test Split</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e519d3-f8a9-4d73-83d6-2d1dc4dce15c",
   "metadata": {},
   "source": [
    "### 1. Load the dataset\n",
    "\n",
    "The dataset is stored in a spreadsheet and consists of columns representing different features in the data. \\\n",
    "The cities column contains the label for each row of data. \\\n",
    "First, the .csv table is read into a DataFrame (DataFrame: an efficient and powerful data storage type in Python). \\\n",
    "The next step is to split the data columnwise into the input data columns and the labels. \n",
    "\n",
    "<u>TASK:</u> To load and preprocess the dataset, please execute all the code cells in [Prepare and analyse the data](#sec_a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bb4fb",
   "metadata": {},
   "source": [
    "### 2. Split the dataset into training and test sets using a 80-20 train-test ratio\n",
    "\n",
    "To test the model, e.g. whether it generalises well to new, unseen data, the data must be split into several datasets. \\\n",
    "In this case, we will split the dataset into two subsets to be used for training and testing the model. \\\n",
    "The train-test ratio should be 80-20, meaning that 80% of the data is in the training set and 20% is in the test set. \\\n",
    "This split is often used for train-test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da87c0",
   "metadata": {},
   "source": [
    "### Plot the distribution of the training and test dataset\n",
    "\n",
    "<u>TASK:</u> Run the train-test split (following two code cells) several times and check the distribution of the data points. \\\n",
    "To do this, run the next two cells multiple times: \n",
    "\n",
    "**Important**: The assignment of data points to the train and test splits is random. \\\n",
    "The seed ensures that the random split is the same for each run of the code cell. \\\n",
    "When you are finished checking the train-test split, please change the code to ``random_seed=True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bad22",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.883938100Z"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = None # change this value to True after executing the next two code cells a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917838fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    filtered_data, binarized_labels, test_size=0.2, seed=random_seed\n",
    ")\n",
    "\n",
    "# Plot train and test datasets\n",
    "scatter_plot_train_test(filtered_data, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "check_value(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b9c76",
   "metadata": {},
   "source": [
    "### 3. Train a simple classification model on the training set\n",
    "\n",
    "In this case, a logistic regression model is trained on the data. \\\n",
    "The logistic regression model is used from scikit-learn, an open source machine learning library. \\\n",
    "The model is first trained on the training data and then used to make predictions on the test data.  \n",
    "\n",
    "The logistic regression model is optimised for classification problems, it can be thought of as a 'decision line', \\\n",
    " or 'plane' used to divide the data points into two or more different classes. \\\n",
    " In our case, the cities are the different classes we want to predict. \\\n",
    " Below is a graph showing the logistic regression 'decision line'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b1292",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.888556200Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "logistic_reg = linear_model.LogisticRegression()\n",
    "\n",
    "# fit the logistic regression model on the training data\n",
    "model = logistic_reg.fit(X_train, y_train.to_numpy().ravel())\n",
    "\n",
    "# now the trained model predicts on the test data\n",
    "y_pred = logistic_reg.predict(X_test)\n",
    "print(\"predicitons:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceee6fb",
   "metadata": {},
   "source": [
    "### Plot the decision boundary and the datapoints of the test dataset\n",
    "\n",
    "This plot visualizes the decision boundary of the model. \\\n",
    "Since the dataset has 5 dimensions, plotting all dimensions directly isnâ€™t feasible. \\\n",
    "Instead, we used Principal Component Analysis (PCA) to reduce the data to a few key dimensions that capture the most significant patterns. \\\n",
    "The axes shown represent combinations of original features that contain the highest variance, \\\n",
    "helping to show the underlying structure of the data in a simpler, two-dimensional view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b1a34",
   "metadata": {},
   "source": [
    "<u>TASK:</u> Plot the test and train datasets with the logistic regression decision line. \\\n",
    "You can do so by changing the code below and use the datasets ``X_train`` and ``y_train`` or ``X_test`` and ``y_test``.\\\n",
    "These are variable names, please enter them without apostrophs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1891a7b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.890777100Z"
    }
   },
   "outputs": [],
   "source": [
    "# change these values:\n",
    "X_plot = None # e.g. X_train or X_test\n",
    "y_plot = None # e.g. y_train or y_test\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS CODE\n",
    "check_plot_values(X_plot, y_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550eee8b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.893024100Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "X_train_2dims = get_principal_components(X_plot, 2)\n",
    "first_col, second_col, xx, yy = prepare_4_plot(X_train_2dims)\n",
    "\n",
    "logistic_reg_plot = linear_model.LogisticRegression()\n",
    "model = logistic_reg_plot.fit(X_train_2dims, y_plot.to_numpy().ravel())\n",
    "\n",
    "plot_decision_boundary_2d(model, xx, yy, first_col, second_col, y_plot, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49113616",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.896268900Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "X_train_3dims, xx, yy, first_col, second_col, third_col = prepare_data(X_plot)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "zz, model = logistic_reg_3d(X_train_3dims, y_plot, xx, yy)\n",
    "# plot the data and the decision boundary\n",
    "plot_decision_boundary_3d(model, xx, yy, zz, y_plot, first_col, second_col, third_col, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eff762",
   "metadata": {},
   "source": [
    "### 4. Evaluate the performance\n",
    "\n",
    "The prediction results from the previous section should be used to evaluate the performance on the test set using commonly used metrics (accuracy, precision, recall and F1-score).\n",
    "\n",
    "<u>Confusion Matrix</u> \\\n",
    "The confusion matrix classifies predictions into four different sections. These classification values are later used to calculate the evaluation metrics.\n",
    "\n",
    "![Confusion Matrix](https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "\n",
    "- True Positives (TP): Cases where the model correctly predicts a positive class (e.g., predicts \"Yes\" when it is indeed \"Yes\").\n",
    "\n",
    "- True Negatives (TN): Cases where the model correctly predicts a negative class (e.g., predicts \"No\" when it is indeed \"No\").\n",
    "\n",
    "- False Positives (FP): Cases where the model incorrectly predicts the positive class (e.g., predicts \"Yes\" when it is actually \"No\"). \n",
    "\n",
    "- False Negatives (FN): Cases where the model incorrectly predicts the negative class (e.g., predicts \"No\" when it is actually \"Yes\").\n",
    "\n",
    "\n",
    "**Criteria for balanced datasets**\n",
    "\n",
    "<ins>Accuracy</ins>\n",
    "\n",
    "Fraction of correct predictions from all predictions.\n",
    "$$\n",
    "  accuracy =\\frac{tn + tp}{tn + fp + fn + tp}\n",
    "$$\n",
    "\n",
    "\n",
    "**Criteria for unbalanced datasets**\n",
    "\n",
    "<ins>Precision</ins>\n",
    "\n",
    "Percentage of correct positive predictions out of all positive predictions made by the model.\n",
    "$$\n",
    "  precision =\\frac{tp}{tp + fp}\n",
    "$$\n",
    "\n",
    "<ins>Recall</ins>\n",
    "\n",
    "Percentage of correct positive predictions out of all positively labelled data. \n",
    "$$\n",
    "  recall =\\frac{tp}{tp + fn}\n",
    "$$\n",
    "\n",
    "<ins>F1-score</ins>\n",
    "\n",
    "The F1-score combines precision and recall. \n",
    "$$\n",
    "  f1 =\\frac{2 * tp}{2 * tp + fp + fn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f49af",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.899101500Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "F1_score = f1_score(y_test, y_pred)\n",
    "print(\"F1-Score: \", F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c0e0a-7050-4d4d-bee6-f94bf8ffc4d5",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578574f5-4a42-4df1-acd6-96a7aff67c5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a id=\"sec_c\"> c) Implementing K-Fold Cross-Validation</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89604c9-f933-4b1f-a3a1-47e8ddd9a349",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "If you run all the code cells above, the data will already be loaded and you won't need to do anything. \\\n",
    "If not, use the code sections provided in [Prepare and analyse the data](#sec:a) to load the data and perform preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2236986-ee96-4bd5-8cb1-ec4ae28c52df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Implement k-fold cross-validation\n",
    "\n",
    "<u>Short introduction to k-fold cross-validation</u> \\\n",
    "K-fold cross-validation is a technique used to evaluate how well a machine learning model performs on new, unseen data. \\\n",
    "For k-fold cross validation, only the training set is used. \\\n",
    "The test set is previously splitted of the whole dataset and set aside until final evaluation. \\\n",
    "The training set is divided into k equal parts, or \"folds\". \\\n",
    "In each round, one fold is kept as the validation set, while the remaining k-1 folds are used to train the model. \\\n",
    "This process is repeated k times, with each fold getting a chance to be the validation set once. \\\n",
    "The model's performance is measured each time, and at the end the results are averaged to give a more reliable measure of accuracy. \\\n",
    "This helps to ensure that the model doesn't overfit to any part of the data, giving a better indication of how it will perform in real-world situations. \n",
    "\n",
    "![k-fold cross validation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*PdwlCactbJf8F8C7sP-3gw.png)\n",
    "\n",
    "<u>TASK:</u> Try different values of k in the range [3, ..., 10] and evaluate the changes on the model results. \\\n",
    "Do this by changing the value of k and running the following three cells of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05020b-1fa2-4685-9da6-a817744f90fb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.902667400Z"
    }
   },
   "outputs": [],
   "source": [
    "k = None # Change this parameter to a number in the range of [3, ..., 10]\n",
    "\n",
    "\n",
    "# DO NOT CHANGE\n",
    "check_k(k)\n",
    "data_folds, label_folds = create_k_folds(X_train, y_train, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6d97d-f37e-4a09-82d4-018f9975823d",
   "metadata": {},
   "source": [
    "### 3. Train a simple classification model and evaluate it k-times\n",
    "\n",
    "A logistic regression model is trained on the data. First, k-fold cross-validation is performed with k = 5, corresponding to an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3019f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.906025600Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "# train the logistic regression model for k times and evaluate\n",
    "scores_k = eval_cross_val(data_folds, label_folds)\n",
    "print_cross_val_result(scores_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bc1d2",
   "metadata": {},
   "source": [
    "### 4. Plot the training and validation accuracy for each fold\n",
    "Underneath, a plot displays the training and validation accuracies for each fold from the most recent training and evaluation process. \\\n",
    "The x-axis represents the fold number (or count of training folds), while the y-axis shows the accuracy values for both training and validation sets. \\\n",
    "This visualization allows you to observe how the model's performance varies across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504893b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.910397100Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "y_min, y_max = get_ymin_ymax_scores([scores_k])\n",
    "plot_acc_per_fold(scores_k, y_min, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c241191",
   "metadata": {},
   "source": [
    "### 5. Plot the mean accuracy for different folds (k = 3, 5, 10)\n",
    "The plot below shows the mean accuracy across different values of (k = 3, 5, 10) from k-fold cross-validation. \\\n",
    "The x-axis represents the number of folds, while the y-axis indicates the mean accuracy achieved for each value of k. \\\n",
    "This plot provides insight into how the choice of k impacts the model's performance.\n",
    "\n",
    "\n",
    "**Important:** Please note the scaling of the y-axis, which indicates accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2f8b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.911482400Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "k_range = [3, 5, 10]\n",
    "\n",
    "k_result_train, k_result_val, k_result_f1 = get_results(X_train, y_train, k_range)\n",
    "y_min, y_max = get_ymin_ymax(k_result_train, k_result_val) \n",
    "train_plot_folds(k_range, k_result_train, k_result_val, k_result_f1, y_lim_min=y_min, y_lim_max=y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122d244-af22-4ce9-9258-59293a5dfd68",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf399fe-948b-4f13-b51a-fab6c58201d5",
   "metadata": {},
   "source": [
    "## <a id=\"sec_d\">d) Stratified K-Fold Cross-Validation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f26dd",
   "metadata": {},
   "source": [
    "<u>Short introduction to stratified k-fold cross-validation</u> \\\n",
    "Stratified k-fold cross-validation is a technique used to evaluate the performance of a model on unseen data, \\\n",
    "while maintaining the same distribution of target classes in each fold as in the original dataset. \\\n",
    "This is particularly important when the dataset is unbalanced. \\\n",
    "This involves dividing the dataset into k equal parts or \"folds\", ensuring that each fold has a similar proportion of classes. \\\n",
    "In each round, one fold is used as the validation set, while the remaining k-1 folds are used to train the model. \\\n",
    "This process is repeated k times, so that each fold is used as a validation set once. \\\n",
    "The model's performance is measured each time and the results are averaged to provide a more reliable estimate of accuracy. \\\n",
    "This technique helps to prevent overfitting, particularly where there is class imbalance, and ensures that the model generalises better to new data.\n",
    "\n",
    "![k-fold cross validation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*PdwlCactbJf8F8C7sP-3gw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b954fe-015c-4b31-b3c5-fea49c06969b",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "Stratified K-fold cross-validation is used for unbalanced datasets. \\\n",
    "The dataset contains the different labels (cities) at different frequencies. \\\n",
    "To create an unbalanced dataset, some randomly drawn data rows with one label value are deleted from the dataset.\n",
    "\n",
    "<u>TASK:</u> Change the ``stratification_factor`` to a value between 0 and 1.\n",
    "\n",
    "The data of city2 will then be cropped by the prozentual amount of the stratification factor. \\\n",
    "This will be done to simulate a stratified data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratification_factor = None # Please fill in a value between 0 and 1\n",
    "# This value is used to delete stratification_factor*100 % of the data, that belongs to the city, which is mapped to the value 1 (city2)\n",
    "\n",
    "\n",
    "# DO NOT CHANGE \n",
    "check_factor(stratification_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704328ca-df18-406f-b11b-e8077fabdbca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.915728800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "# to create an imbalanced dataset, 80 % of the data rows will be dropped, where the mapping is 1 (previously assigned city2)\n",
    "y_train_cities = binarize_labels_rev(y_train, mapping)\n",
    "normalized_data_min, labels_min = minorize_label(\n",
    "    X_train, y_train_cities, values[1], stratification_factor\n",
    ")\n",
    "\n",
    "\n",
    "filtered_data_str, filtered_labels = filter_label_values(\n",
    "    normalized_data_min, labels_min, values\n",
    ")\n",
    "\n",
    "print(\"\\n mapping: \", mapping)\n",
    "binarized_labels_str = binarize_labels(filtered_labels, mapping)\n",
    "\n",
    "if filtered_data_str.shape[0] != binarized_labels_str.shape[0]:\n",
    "    raise Exception(\"Error: data and labels do not have the same number of rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d02442",
   "metadata": {},
   "source": [
    "### 2. Determine the number of classes and samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57accec0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.916739300Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "label_counts = filtered_labels[filtered_labels.columns[0]].value_counts().reset_index()\n",
    "label_counts.columns = [\"City\", \"Frequency\"]\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b67ca0",
   "metadata": {},
   "source": [
    "### 3. Visualize the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd280095",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.917803100Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "label_counts.plot(kind=\"bar\", rot=0)\n",
    "plt.title(\"Histogram of Data Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933697b",
   "metadata": {},
   "source": [
    "### 4. Implement stratified K-fold cross-validation \n",
    "Stratified k-fold cross validation ensures, that each fold has the original class proportions. \n",
    "\n",
    "<u>TASK:</u> Compare the performance of the model when using different values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d839318",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.920190100Z"
    }
   },
   "outputs": [],
   "source": [
    "k = None # Change k to a range of [3, ..., 10]\n",
    "\n",
    "\n",
    "# DO NOT CHANGE\n",
    "check_k(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77527d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE \n",
    "data_folds_k_str, label_folds_k_str = create_k_folds(\n",
    "    filtered_data_str, binarized_labels_str, k, StratifiedKFold\n",
    ")\n",
    "scores_k_str = eval_cross_val(data_folds_k_str, label_folds_k_str)\n",
    "print_cross_val_result(scores_k_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1114b5",
   "metadata": {},
   "source": [
    "### 5. Plot the training and validation accuracy for each fold\n",
    "Underneath, a plot displays the training and validation accuracies for each fold from the most recent training and evaluation process. \\\n",
    "The x-axis represents the fold number (or count of training folds), while the y-axis shows the accuracy values for both training and validation sets. \\\n",
    "This visualization allows you to observe how the model's performance varies across folds.\n",
    "\n",
    "The first plot shows the imbalanced dataset with stratified k-fold cross validation, the second plot uses normal k-fold cross validation.\n",
    "\n",
    "<u>TASK:</u> Compare both plots, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad563846",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.921220900Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "data_folds_k_imbalanced, label_folds_k_imbalanced = create_k_folds(\n",
    "    filtered_data_str, binarized_labels_str, k, KFold\n",
    ")\n",
    "scores_k_imbalanced = eval_cross_val(data_folds_k_imbalanced, label_folds_k_imbalanced)\n",
    "\n",
    "y_min, y_max = get_ymin_ymax_scores([scores_k_str, scores_k_imbalanced])\n",
    "\n",
    "text_str = \"Imbalanced dataset, Stratified k-fold cross validation\"\n",
    "plot_acc_per_fold(scores_k_str, y_min, y_max, text_str)\n",
    "\n",
    "text_kf = \"Imbalanced dataset, basic k-fold cross validation\"\n",
    "plot_acc_per_fold(scores_k_imbalanced, y_min, y_max, text_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e6365",
   "metadata": {},
   "source": [
    "### 6. Plot the mean accuracy for different folds (k = 3, 5, 10)\n",
    "The plot below shows the mean accuracy across different values of (k = 3, 5, 10) from k-fold cross-validation. \\\n",
    "The x-axis represents the number of folds, while the y-axis indicates the mean accuracy achieved for each value of k. \\\n",
    "This plot provides insight into how the choice of k impacts the model's performance.\n",
    "\n",
    "The first plot shows the imbalanced dataset with stratified k-fold cross validation, the second plot uses normal k-fold cross validation.\n",
    "\n",
    "**Important:** Please note the scaling of the y-axis, which indicates accuracy!\n",
    "\n",
    "<u>TASK:</u> Compare the results with regular k-fold cross-validation and discuss the implications for performance metrics such as F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e52b6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.921377600Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "k_range = [3, 5, 7, 10, 20]\n",
    "\n",
    "k_result_train_strkfold, k_result_val_strkfold, k_result_f1_strkfold = get_results(filtered_data_str, binarized_labels_str, k_range, fold=StratifiedKFold)\n",
    "k_result_train_kfold, k_result_val_kfold, k_result_f1_kfold = get_results(filtered_data_str, binarized_labels_str, k_range, fold=KFold)\n",
    "\n",
    "y_min, y_max = get_ymin_ymax(k_result_train_strkfold, k_result_val_strkfold)\n",
    "\n",
    "train_plot_folds(\n",
    "    k_range, k_result_train_strkfold, k_result_val_strkfold, k_result_f1_strkfold, y_min, y_max, text=text_str\n",
    ")\n",
    "\n",
    "train_plot_folds(k_range, k_result_train_kfold, k_result_val_kfold, k_result_f1_kfold, y_min, y_max, text=text_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb090de",
   "metadata": {},
   "source": [
    "### 7. Plot the mean F1-Score for different folds (k = 3, 5, 10)\n",
    "The plot below shows the mean F1-Score across different values of (k = 3, 5, 10) from k-fold cross-validation. \\\n",
    "The x-axis represents the number of folds, while the y-axis indicates the mean F1-Score achieved for each value of k. \\\n",
    "This plot provides insight into how the choice of k impacts the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb3ece",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.922547400Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE\n",
    "k_range = [3, 5, 7, 10, 20]\n",
    "\n",
    "k_result_train_strkfold, k_result_val_strkfold, k_result_f1_strkfold = get_results(filtered_data_str, binarized_labels_str, k_range, fold=StratifiedKFold)\n",
    "k_result_train_kfold, k_result_val_kfold, k_result_f1_kfold = get_results(filtered_data_str, binarized_labels_str, k_range, fold=KFold)\n",
    "\n",
    "y_min, y_max = get_ymin_ymax(k_result_f1_strkfold, k_result_f1_kfold)\n",
    "\n",
    "train_plot_folds(\n",
    "    k_range, k_result_train_strkfold, k_result_val_strkfold, k_result_f1_strkfold, y_min, y_max, mode = 'F1-Score', text=text_str\n",
    ")\n",
    "\n",
    "train_plot_folds(k_range, k_result_train_kfold, k_result_val_kfold, k_result_f1_kfold, y_min, y_max, mode = 'F1-Score', text=text_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad96bb4-71b1-4edf-b6e6-2080293f50a3",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331642c",
   "metadata": {},
   "source": [
    "## <a id=\"sec_e\">Bonus) Plot the data</a>\n",
    "A dataset has often multiple features (columns). \\\n",
    "Plotting the data in such a high dimensional feature space (>3) is not possible. \\\n",
    "For plotting, but also for data processing, it is sometimes beneficial to convert \\\n",
    "the dataset into a lower dimensional feature space (reduce the amount of columns). \\\n",
    "This can be done by using different dimensionality reduction techniques.\n",
    "\n",
    "The next sections provide plots of the data using dimensionality reduction techniques. \\\n",
    "If you are interested you can voluntarily work on these tasks too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b1b72",
   "metadata": {},
   "source": [
    "The data will be plottet along two features. \\\n",
    "These features are containing the most information to classify the data. \\\n",
    "The different cities are marked with the different colors. \n",
    "\n",
    "In this section we work with a reduced dataset, which only contains the following features:\n",
    "- ``temp``\n",
    "- ``dew``\n",
    "- ``humidity``\n",
    "- ``pressure``\n",
    "- ``sunriseEpoch``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "columns_to_drop = [\n",
    "    \"datetimeEpoch\", \"datetime\", \"Month\", \"preciptype\", \"Health_Risk_Score\", \"sunrise\", \"sunset\",\n",
    "    \"sunsetEpoch\", \"conditions\", \"description\", \"icon\", \"stations\", \"source\", \"Season\",\n",
    "    \"Day_of_Week\", \"feelslikemin\", \"feelslikemax\", \"feelslike\", \"Condition_Code\", \"Is_Weekend\",\n",
    "    \"tempmax\", \"tempmin\", \"severerisk\", \"Temp_Range\", \"Heat_Index\", \"precip\", \"precipprob\", \"precipcover\",\n",
    "    \"snow\", \"snowdepth\", \"windgust\", \"windspeed\", \"winddir\", \"Severity_Score\", \"cloudcover\", \n",
    "    \"visibility\", \"solarradiation\", \"solarenergy\", \"uvindex\", \"moonphase\", \"City\",\n",
    "]\n",
    "labels = orig_data[\"City\"].to_frame()\n",
    "data = orig_data.drop(columns=columns_to_drop)\n",
    "normalized_data = (data - data.mean()) / data.std()\n",
    "normalized_data = normalized_data.dropna(axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f473c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.923730200Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "first_col = normalized_data[\"humidity\"].to_frame()\n",
    "second_col = normalized_data[\"temp\"].to_frame()\n",
    "\n",
    "cities = labels[labels.columns[0]].unique()\n",
    "\n",
    "mapping_all_cities = create_city_mapping(cities)\n",
    "all_labels_bin = binarize_labels(labels, mapping_all_cities)\n",
    "\n",
    "title = \"Data points along the two most important features\"\n",
    "scatter_plot(first_col, second_col, all_labels_bin, title)\n",
    "create_legend(mapping_all_cities, all_labels_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b70628",
   "metadata": {},
   "source": [
    "The data will be plottet along the two most impotant features. \\\n",
    "These features are containing the most information to classify the data. \\\n",
    "The different cities are marked with the different colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1995e29",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.924875800Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "# PCA\n",
    "\n",
    "X_train_2dims = get_principal_components(normalized_data, 2)\n",
    "first_col, second_col, _, _ = prepare_4_plot(X_train_2dims)\n",
    "\n",
    "cities = labels[labels.columns[0]].unique()\n",
    "\n",
    "mapping_all_cities = create_city_mapping(cities)\n",
    "all_labels_bin = binarize_labels(labels, mapping_all_cities)\n",
    "\n",
    "title = \"Data points along the two most important features\"\n",
    "scatter_plot(first_col, second_col, all_labels_bin, title)\n",
    "create_legend(mapping_all_cities, all_labels_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd2377",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.925977700Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE   \n",
    "# TSNE\n",
    "\n",
    "# converts similarities into joint probabilities and minimizes kullback leibler\n",
    "# divergence between joint probabilites of the low dimensional space\n",
    "X_train_2dims_tsne = TSNE(n_components=2).fit_transform(normalized_data)\n",
    "X_train_2dims_tsne = pd.DataFrame(\n",
    "    X_train_2dims_tsne, columns=[\"first dim tsne\", \"second dim tsne\"]\n",
    ")\n",
    "first_col, second_col, _, _ = prepare_4_plot(X_train_2dims_tsne)\n",
    "\n",
    "cities = labels[labels.columns[0]].unique()\n",
    "\n",
    "mapping_all_cities = create_city_mapping(cities)\n",
    "all_labels_bin = binarize_labels(labels, mapping_all_cities)\n",
    "\n",
    "title = \"Data points along the two most important features\"\n",
    "scatter_plot(first_col, second_col, all_labels_bin, title)\n",
    "create_legend(mapping_all_cities, all_labels_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4fca0",
   "metadata": {},
   "source": [
    "Underneath, there is a 3 dimensional plot of the data, you can try out different plot modes like PCA (aligning the axis, that the plot shows as much variance in the data as possible) and the original axes.\n",
    "\n",
    "<u>TASK:</u> Try different plot modes and dimensions of the data and inspect, how the plot changes. \n",
    "For mode you can insert the values: \"orig axes\", \"tsne\", and \"pca\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44ce4c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-11T06:20:17.929305100Z"
    }
   },
   "outputs": [],
   "source": [
    "# change to these modes: \"orig axes\", \"tsne\", and \"pca\"\n",
    "mode = \"pca\"\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS CODE\n",
    "title = f\"Data points along the three most important features - {mode}\"\n",
    "plot_data_3dim(normalized_data, labels, title, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40cf25",
   "metadata": {},
   "source": [
    "[Back](#sec_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19888f1",
   "metadata": {},
   "source": [
    "### Used images\n",
    "[https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z54JgbS4DUwWSknhDCvNTQ.png](https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "[https://miro.medium.com/v2/resize:fit:640/format:webp/1*PdwlCactbJf8F8C7sP-3gw.png](https://miro.medium.com/v2/resize:fit:640/format:webp/1*PdwlCactbJf8F8C7sP-3gw.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
