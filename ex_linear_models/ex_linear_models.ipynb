{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554b963d-be9e-41b9-b56d-bff5879007b8",
   "metadata": {},
   "source": [
    "# Exercise - Data Modeling of Objects in Free Fall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251029f-99c6-44fe-abb5-d6f62a27a914",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This is an exercise about linear models. Their limitations will be identified and solutions based on non-linear models will be explored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06efc40-1457-4515-9787-b46a11119085",
   "metadata": {},
   "source": [
    "## <a id=\"sec:toc\">Content</a>\n",
    "\n",
    "[Learning Objectives](#sec_0)\n",
    "\n",
    "[a) Data Exploration and Model Requirement Identification](#sec_a)\n",
    "\n",
    "[b) Identify the Limitations of Linear Models](#sec_b)\n",
    "\n",
    "[c) Explore a Non-Linear Mixture of Experts Models](#sec_c)\n",
    "\n",
    "[d) Explore Locally Weighted Regression (LWR)](#sec_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee46f1",
   "metadata": {},
   "source": [
    "### <a id=\"sec_0\">Learning Objectives</a>\n",
    "\n",
    "* Indicate relationships in data\n",
    "* Use Error metrics (e.g., mean squared error) and understand their meaning\n",
    "* Evaluate linear models and mixture models\n",
    "* Apply locally weighted regression and analyze how the bandwidth parameter affects the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0009ca-fc9e-4114-ad69-fdf0cf201fe7",
   "metadata": {},
   "source": [
    "Import the needed libaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e53d50-ef99-4696-bfe4-8e7ac575ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac634356-fbd8-47ff-a16c-d68b9a03ceb9",
   "metadata": {},
   "source": [
    "## <a id=\"sec:a\">a) Data Exploration and Model Requirement Identification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6c5b8",
   "metadata": {},
   "source": [
    "In the following part of this notebook we are going to have a look at our data and plot it. \\\n",
    "Try to find connections between the different physical quantities (fall time, height and weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebe65e-f4fc-425b-992a-472ff6da09da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Method to read a file into a dataset\n",
    "def read_file_into_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:  # Ensure the correct file extension\n",
    "        # Read all lines, but skip the first line (the header)\n",
    "        lines = file.readlines()[1:]  # Skip the first line (header)\n",
    "\n",
    "        # Create the dataset from the remaining lines\n",
    "        for line in lines:\n",
    "            # Split the line by commas and convert values to float\n",
    "            fall_time, weight, height = line.strip().split(\",\")\n",
    "            dataset.append([float(fall_time), float(weight), float(height)])  # Append both values as a list of floats\n",
    "    return dataset\n",
    "\n",
    "# Scatter plot of Fall Time vs Height (m) or Fall Time vs Weight (g)\n",
    "def plot_fall_time_vs_y(df, y_axis=\"Height (m)\"):\n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"Fall Time (s)\"], df[y_axis], color='blue', marker='o', label=\"Measured Data\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f'Scatter Plot of Fall Time vs {y_axis}')\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.xlabel('Fall Time (s)')\n",
    "    plt.show()\n",
    "\n",
    "def plot_fall_time_vs_height_weight(df):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Scatter plot for Fall Time vs Height vs Weight\n",
    "    ax.scatter(df[\"Height (m)\"], df[\"Weight (g)\"], df[\"Fall Time (s)\"], color='blue', marker='o', label=\"Measured Data\")\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Height (m)')\n",
    "    ax.set_ylabel('Weight (g)')\n",
    "    ax.set_zlabel('Fall Time (s)')\n",
    "    ax.set_title('3D Scatter Plot of Fall Time vs Height and Weight')\n",
    "    \n",
    "    # Show legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.view_init(elev=30, azim=140)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def check_value(x, param_name):\n",
    "    if x is None:\n",
    "        print(f\"x_axis is still None. Please enter a valid value for {param_name}.\")\n",
    "        raise Exception(\"Execution stopped because x_axis is not defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe695b31",
   "metadata": {},
   "source": [
    "### 1. Load & display the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806278f-6736-411f-a876-6fae2b9caf61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the provided dataset containing measurements of falling times of objects with different weights from different heights\n",
    "dataset = read_file_into_dataset(\"falltimes-weight-height-multiple-measurements.csv\")\n",
    "\n",
    "# Display the DataFrame in a tabular form\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "df = pd.DataFrame(dataset, columns=[\"Fall Time (s)\", \"Weight (g)\", \"Height (m)\"])\n",
    "print(df)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a5189",
   "metadata": {},
   "source": [
    "### 2. Visualize scatter plot of Fall Time vs. Height or Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0882f4",
   "metadata": {},
   "source": [
    "<u>TASK</u>: In the code cell below you can change the y_axis parameter to be able to better examine the relations between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f4ef5-8109-46e4-bf3a-ac1dd06c9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis = None   # CHANGE y_axis variable from None to either \"Height (m)\" or \"Weight (g)\"\n",
    "                # Include the quotation marks\n",
    "\n",
    "# DO NOT CHANGE THE CODE BELOW\n",
    "check_value(y_axis, \"y_axis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428db861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatter plot of Fall Time vs. Height or Weight\n",
    "plot_fall_time_vs_y(df, y_axis = y_axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9dae6c",
   "metadata": {},
   "source": [
    "### 3. Visualize 3D scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 3D scatter plot\n",
    "plot_fall_time_vs_height_weight(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e645d06c-707c-41cd-b1b3-5aa2df5f4764",
   "metadata": {},
   "source": [
    "## <a id=\"sec:b\">Identify the Limitations of Linear Models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb42aaf",
   "metadata": {},
   "source": [
    "Now we will fit a linear model to the data. It will use the heights from which objects are dropped to try to predict the fall times. \\\n",
    "We will have a look at the predictions of this model in a plot. \\\n",
    "We will also compute the MSE of the predicted data. \n",
    "The mean squared error (MSE) is a metric that calculates \\\n",
    "the average of the squares of the differences between predicted and actual values, \\\n",
    "providing a measure of the accuracy of a model, with smaller values indicating better fit.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3cf54",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Reg_Line.png\" width=\"520\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d141c1-899e-4cab-bf0e-e832d83fc78a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Train linear regression model and return model coefficients\n",
    "def train_linear_regression(df):\n",
    "    lin_reg = LinearRegression()\n",
    "    X = df[\"Height (m)\"].values.reshape(-1, 1)  # Features (Weight)\n",
    "    y = df[\"Fall Time (s)\"]  # Target (Fall Time)\n",
    "\n",
    "    # Split dataset into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the linear regression model\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Get the model parameters (c1, c2)\n",
    "    print(f\"Model Coefficients: c1 (bias) = {lin_reg.intercept_}, c2 (slope) = {lin_reg.coef_[0]}\")\n",
    "\n",
    "    # Return the trained model and the split data\n",
    "    return lin_reg, X_test, y_test\n",
    "\n",
    "# Calculate and print predicted and real accelerations\n",
    "def calculate_and_print_accelerations(df, y_test, y_pred):\n",
    "    # Use the test data weights and heights for this\n",
    "    test_heights = df[\"Height (m)\"][y_test.index].values  # Heights corresponding to the test set\n",
    "\n",
    "    # Calculate predicted and real accelerations using a = h / t^2\n",
    "    acceleration_pred = 2 * test_heights / (y_pred ** 2)\n",
    "    acceleration_measured = 2 * test_heights / (y_test.values ** 2)\n",
    "\n",
    "    print(f\"Predicted Accelerations:\\n {acceleration_pred}\")\n",
    "    print(f\"Measured Accelerations:\\n {acceleration_measured}\")\n",
    "\n",
    "# Visualize true vs. predicted fall times and model predictions\n",
    "def visualize_predictions(df, y_test, y_pred, lin_reg):\n",
    "    heights = df[\"Height (m)\"][y_test.index]\n",
    "    # Scatter plot: True vs. Predicted fall times (Fall Time vs. Height)\n",
    "    plt.figure(num=\"True vs. Predicted fall times\")\n",
    "    plt.scatter(heights, y_test, color='blue', marker='o', label=\"True Measurements\")\n",
    "    plt.scatter(heights, y_pred, color='red', marker='x', label=\"Predicted Fall Times\")\n",
    "    plt.title(\"True vs. Predicted Fall Times (vs. Height)\")\n",
    "    plt.xlabel(\"Height (m)\")\n",
    "    plt.ylabel(\"Fall Time (s)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e84160",
   "metadata": {},
   "source": [
    "### 1. Train the linear regression model \n",
    "\n",
    "Training means that we try to adjust the parameters of our linear model so that it fits the data as well as possible. \\\n",
    "The linear model has the form:\n",
    "$y = c_1*x + c_2$ \\\n",
    "We automatically choose $c_1$ & $c_2$ so that the line matches the data closely. So we want to achieve a low MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecbfdc-861c-437d-820a-8da500f8e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "lin_reg, X_test, y_test = train_linear_regression(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866420c",
   "metadata": {},
   "source": [
    "### 2. Predict fall times for the test set and calculate accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec68dd-aac0-4e68-87f5-c64990d06600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict fall times for the test set and calculate accelerations\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "calculate_and_print_accelerations(df, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe2f53",
   "metadata": {},
   "source": [
    "### 3. Visualize predictions and linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12a682-3398-49a6-a8ae-a751800543f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions and linear model\n",
    "visualize_predictions(df, y_test, y_pred, lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c05a7",
   "metadata": {},
   "source": [
    "### 4. Calculate the Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67021058-ea51-4adc-918f-fdc12f94df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE) between predicted and real fall times: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1cf430-5143-4a6b-b827-847dfd040830",
   "metadata": {},
   "source": [
    "## <a id=\"sec:c\">c) Explore a Non-Linear Mixture of Expert Models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b97b2",
   "metadata": {},
   "source": [
    "In this chapter we will split our data into multiple subsets and train separate models for each of these subsets. \\\n",
    "The data will be split by height. We will also compute the total MSE to see if multiple models perform better than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92e8d1-2788-4e53-abe6-7d7df34cbd32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Function to split data using a simple decision tree based on height\n",
    "def split_data_by_height(df, num_splits):\n",
    "    # Split the dataset into `num_splits` subsets based on height\n",
    "    min_height = df[\"Height (m)\"].min()\n",
    "    max_height = df[\"Height (m)\"].max()\n",
    "\n",
    "    # Define height thresholds based on the number of splits\n",
    "    thresholds = np.linspace(min_height, max_height, num_splits + 1)\n",
    "\n",
    "    subsets = []\n",
    "    for i in range(num_splits):\n",
    "        if i < num_splits - 1:\n",
    "            # Include lower bound and exclude upper bound for all but the last subset\n",
    "            subset = df[(df[\"Height (m)\"] >= thresholds[i]) & (df[\"Height (m)\"] < thresholds[i + 1])]\n",
    "        else:\n",
    "            # Include both bounds for the last subset\n",
    "            subset = df[(df[\"Height (m)\"] >= thresholds[i]) & (df[\"Height (m)\"] <= thresholds[i + 1])]\n",
    "        subsets.append(subset)\n",
    "\n",
    "    return subsets, thresholds\n",
    "\n",
    "\n",
    "# Function to train linear models with train-test splits for each subset of the data\n",
    "def train_linear_models_with_split(subsets, test_size=0.2, random_state=42):\n",
    "    models = []\n",
    "    subset_data = []  # To store the train and test sets for each subset\n",
    "\n",
    "    for subset in subsets:\n",
    "        X = subset[\"Height (m)\"].values.reshape(-1, 1)\n",
    "        y = subset[\"Fall Time (s)\"]\n",
    "\n",
    "        # Perform train-test split for each subset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Train the linear regression model on the training data\n",
    "        lin_reg = LinearRegression()\n",
    "        lin_reg.fit(X_train, y_train)\n",
    "\n",
    "        # Store the model and train-test data\n",
    "        heights_test = subset.loc[y_test.index, \"Height (m)\"]  # Ensure heights correspond only to the test data\n",
    "        models.append(lin_reg)\n",
    "        subset_data.append({\n",
    "            \"X_train\": X_train, \"X_test\": X_test,\n",
    "            \"y_train\": y_train, \"y_test\": y_test,\n",
    "            \"heights_test\": heights_test.values  # Store only the heights corresponding to the test set\n",
    "        })\n",
    "\n",
    "    return models, subset_data\n",
    "\n",
    "\n",
    "# Function to compute predictions and errors for the test sets of each subset\n",
    "def compute_predictions_and_errors_with_split(subset_data, models):\n",
    "    total_weighted_mse = 0\n",
    "    total_data_points = 0\n",
    "\n",
    "    # Define reversed viridis colormap for actual values and inferno for predicted values\n",
    "    actual_colors = np.flip(plt.cm.viridis(np.linspace(0, 1, len(subset_data))), axis=0)\n",
    "    predicted_colors = plt.cm.inferno(np.linspace(0, 1, len(subset_data)))\n",
    "\n",
    "    for i, data in enumerate(subset_data):\n",
    "        X_test = data[\"X_test\"]\n",
    "        y_test = data[\"y_test\"]\n",
    "        heights_test = data[\"heights_test\"]\n",
    "\n",
    "        # Predict the fall times using the trained model\n",
    "        y_pred = models[i].predict(X_test)\n",
    "\n",
    "        # Calculate the Mean Squared Error (MSE) for the test set\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        n_data_points = len(y_test)  # Number of data points in this subset's test set\n",
    "\n",
    "        # Accumulate weighted MSE: n_data_points * mse\n",
    "        total_weighted_mse += n_data_points * mse\n",
    "        total_data_points += n_data_points  # Keep track of total number of data points\n",
    "\n",
    "        print(f\"Subset {i + 1} MSE: {mse}\")\n",
    "\n",
    "        # Plot the true and predicted values for each subset (Fall Time vs. Height)\n",
    "        plt.figure()\n",
    "        plt.scatter(heights_test, y_test, color=actual_colors[i], marker='o', \n",
    "                    label=f\"True Data (Subset {i + 1})\", alpha=0.8)\n",
    "        plt.scatter(heights_test, y_pred, color=predicted_colors[i], marker='x', \n",
    "                    label=f\"Predicted Data (Subset {i + 1})\", alpha=0.8)\n",
    "        plt.title(f\"Subset {i + 1}: True vs. Predicted Fall Times (Test Set)\")\n",
    "        plt.xlabel(\"Height (m)\")\n",
    "        plt.ylabel(\"Fall Time (s)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"lower right\", fontsize=9)\n",
    "        plt.ylim(-0.5, 5.1)\n",
    "        plt.xlim(-0.5, 101)\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate the combined MSE using the total weighted MSE and total number of data points\n",
    "    combined_mse = total_weighted_mse / total_data_points\n",
    "    print(f\"Total (Weighted) MSE across all subsets: {combined_mse}\")\n",
    "\n",
    "# Function to plot Fall Time vs Height (True vs Predicted)\n",
    "def plot_falltime_vs_height(subset_data, models):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Define reversed viridis colormap for actual values and inferno for predicted values\n",
    "    actual_colors = np.flip(plt.cm.viridis(np.linspace(0, 1, len(subset_data))), axis=0)\n",
    "    predicted_colors = plt.cm.inferno(np.linspace(0, 1, len(subset_data)))\n",
    "\n",
    "    for i, data in enumerate(subset_data):\n",
    "        heights_test = data[\"heights_test\"]  # Heights corresponding only to the test set\n",
    "        X_test = data[\"X_test\"]\n",
    "        y_test = data[\"y_test\"]\n",
    "\n",
    "        # Predict the fall times using the trained model\n",
    "        y_pred = models[i].predict(X_test)\n",
    "        \n",
    "        # Plot true fall times (reversed viridis colormap)\n",
    "        plt.scatter(heights_test, y_test, color=actual_colors[i], marker='o', \n",
    "                    label=f\"True Data (Subset {i + 1})\", alpha=0.8)\n",
    "        \n",
    "        # Plot predicted fall times (inferno colormap)\n",
    "        plt.scatter(heights_test, y_pred, color=predicted_colors[i], marker='x', \n",
    "                    label=f\"Predicted Data (Subset {i + 1})\", alpha=0.8)\n",
    "\n",
    "    plt.title(\"True vs Predicted Fall Times for All Models\")\n",
    "    plt.xlabel(\"Height (m)\")\n",
    "    plt.ylabel(\"Fall Time (s)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\", fontsize=9)\n",
    "    plt.ylim(-0.5, 5.1)\n",
    "    plt.xlim(-0.5, 101)\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bb612",
   "metadata": {},
   "source": [
    "### 1. Split data by height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302c167",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can decide the number of subsets by changing the parameter ``num_splits``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac48cbd-8ecc-4c67-ac40-58be63accc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = None # CHANGE num_splits variable from None to natural numbers in the interval [1, 34]\n",
    "\n",
    "# DO NOT CHANGE THE CODE BELOW\n",
    "check_value(num_splits, \"num_splits\")\n",
    "# Split data by height using the decision tree with a specified number of splits\n",
    "subsets, thresholds = split_data_by_height(df, num_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fc4e9",
   "metadata": {},
   "source": [
    "### 2. Train individual linear models for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d47e4-2911-4ca6-9d17-7810ad725576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train individual linear models for each subset with train-test splits\n",
    "models, subset_data = train_linear_models_with_split(subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ce4e4",
   "metadata": {},
   "source": [
    "### 3. Compute predictions and errors for each test set of the subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f76588-f359-4917-8ba0-d33a47934c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions and errors for each test set of the subsets\n",
    "compute_predictions_and_errors_with_split(subset_data, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f16ed6",
   "metadata": {},
   "source": [
    "### 4. Plot Fall Time vs Height (True vs Predicted) for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091a8a3-66ca-42ed-816e-a291d8a688d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fall Time vs Height (True vs Predicted) for all models\n",
    "plot_falltime_vs_height(subset_data, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cab766-531e-4010-a026-390cc10c9095",
   "metadata": {},
   "source": [
    "## <a id=\"sec:c\">Explore Locally Weighted Regression (LWR)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14425538",
   "metadata": {},
   "source": [
    "In this part of the notebook we will make predictions using a locally weighted regression \\\n",
    "model for one datapoint of each of the previously defined subsets. \\\n",
    "Locally weighted regression fits a model to a target point by giving nearby data \\\n",
    "points more influence than those further away. \\\n",
    "In this example of LWR these distances between the training data points and the test data point \\\n",
    "are computed using the heights from which our objects fall. \\\n",
    "The \"weights\" are computed using a gaussian normal distribution centered at the target point \\\n",
    "(the data for which we are trying to make a prediction). \\\n",
    "The \"weights\" are a measure for how strong a training data point will influence the prediction.\\\n",
    "Do not mistake these weights with the masses of the objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3e1c3-bd45-4123-ba56-82efc5ad65e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# Function to compute distances and importance weights for test samples (only based on training weights)\n",
    "def compute_distances_and_weights(train, test, bandwidth=1.0):\n",
    "    # Extract height values for distance computation (ignore weight)\n",
    "    train_weights = train[[\"Height (m)\"]].values\n",
    "    test_weights = test[[\"Height (m)\"]].values\n",
    "\n",
    "    # Compute Euclidean distances between test and training weights\n",
    "    distances = cdist(test_weights, train_weights, metric='euclidean').flatten()\n",
    "\n",
    "    # Compute Gaussian kernel weights based on the distances\n",
    "    weights = np.exp(-distances ** 2 / (2 * bandwidth ** 2))\n",
    "\n",
    "    # Normalize the weights so they sum to 1\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    return distances, weights\n",
    "\n",
    "\n",
    "# Function to predict fall time using weighted linear regression (based only on weights and fall times)\n",
    "def weighted_linear_regression(train, test, weights):\n",
    "    # Prepare the training data (use weights and fall times)\n",
    "    X_train = train[[\"Height (m)\"]].values\n",
    "    y_train = train[\"Fall Time (s)\"].values\n",
    "\n",
    "    # Add a column of ones to include the bias term\n",
    "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]  # Now X_train is N x 2\n",
    "\n",
    "    # Compute the weighted linear regression using the normal equation\n",
    "    W = np.diag(weights)  # Diagonal weight matrix\n",
    "    XTX = X_train.T @ W @ X_train  # Now XTX is 2 x 2\n",
    "    XTy = X_train.T @ W @ y_train  # Now XTy is 2 x 1\n",
    "\n",
    "    # Solve for theta (linear regression coefficients)\n",
    "    theta = np.linalg.inv(XTX) @ XTy\n",
    "    #print(\"theta =\", theta)\n",
    "\n",
    "    # Prepare test data (add bias term)\n",
    "    X_test = test[[\"Height (m)\"]].values\n",
    "    X_test = np.c_[np.ones(X_test.shape[0]), X_test]  # Now X_test is M x 2\n",
    "\n",
    "    # Predict the fall time for the test data\n",
    "    y_pred = X_test @ theta\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def visualize_all_without_predictions(df, num_splits, seed=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Split the data into subsets based on height (for splitting only, height is ignored afterward)\n",
    "    subsets, thresholds = split_data_by_height(df, num_splits)\n",
    "\n",
    "    # Dictionary to store the selected test samples for later use\n",
    "    selected_test_samples = {}\n",
    "\n",
    "    # Define a color map for the subsets to ensure distinct colors\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, num_splits))\n",
    "\n",
    "    # Iterate over each subset (each expert) and visualize without predictions\n",
    "    for i, subset in enumerate(subsets):\n",
    "        if subset.empty:\n",
    "            continue  # Skip empty subsets\n",
    "\n",
    "        # Extract as many samples as the subset index (i + 1) and sort them by index\n",
    "        extracted_samples = subset.sample(n=(i + 1), random_state=seed)\n",
    "    \n",
    "        # Always select the i-th sample as the test sample (0-based index)\n",
    "        test_sample = extracted_samples.iloc[[i]]\n",
    "        training_samples = subset.drop(test_sample.index)  # Use the rest as training data\n",
    "\n",
    "        # Store the test sample for later use\n",
    "        selected_test_samples[i] = test_sample\n",
    "\n",
    "        # Plot the training data points (fall times vs heights)\n",
    "        plt.scatter(training_samples[\"Height (m)\"], training_samples[\"Fall Time (s)\"], \n",
    "                    color=colors[i], label=f\"Training Data (Subset {i + 1})\")\n",
    "\n",
    "        # Plot the test data points (fall times vs heights)\n",
    "        plt.scatter(test_sample[\"Height (m)\"], test_sample[\"Fall Time (s)\"], marker='x',\n",
    "                    color='green', label=f\"Test Data (Subset {i + 1})\", s=150)\n",
    "    \n",
    "    plt.title(f\"Training and Test Fall Times (Fall Time vs Height)\")\n",
    "    plt.xlabel(\"Height (m)\")\n",
    "    plt.ylabel(\"Fall Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Return the selected test samples\n",
    "    return selected_test_samples\n",
    "\n",
    "# Function to compute MSE and print it\n",
    "def compute_mse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    return mse\n",
    "\n",
    "# Function to visualize all training, test data, and predictions in one plot, with MSE calculation\n",
    "def visualize_all_with_predictions(df, num_splits, selected_test_samples, bandwidth=1.0):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Define a color map for the subsets to ensure distinct colors\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, num_splits))\n",
    "\n",
    "    # Split the data into subsets based on height (for splitting only, height is ignored afterward)\n",
    "    subsets, thresholds = split_data_by_height(df, num_splits)\n",
    "\n",
    "    # Initialize list to store actual and predicted values for MSE computation across all subsets\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "\n",
    "    # Iterate over each subset (each expert) and visualize with predictions\n",
    "    for i, subset in enumerate(subsets):\n",
    "        if subset.empty:\n",
    "            continue  # Skip empty subsets\n",
    "\n",
    "        # Retrieve the test sample that was used in the first visualization\n",
    "        test_sample = selected_test_samples.get(i)\n",
    "        if test_sample is None:\n",
    "            continue  # Skip if no test sample was recorded\n",
    "\n",
    "        # Use the entire dataset (excluding the test sample itself) as training data\n",
    "        training_samples = df.drop(test_sample.index)  # All data except the selected test sample\n",
    "\n",
    "        # Compute distances and weights (based on the entire dataset, excluding the test sample itself)\n",
    "        distances, weights = compute_distances_and_weights(training_samples, test_sample, bandwidth=bandwidth)\n",
    "\n",
    "        # Predict the fall time using weighted linear regression (based only on weights and fall times)\n",
    "        y_pred = weighted_linear_regression(training_samples, test_sample, weights)\n",
    "\n",
    "        # Store the actual and predicted values for MSE calculation\n",
    "        actual_values.extend(test_sample[\"Fall Time (s)\"])\n",
    "        predicted_values.extend(y_pred)\n",
    "\n",
    "        # Plot the training data points from the current subset (fall times vs heights)\n",
    "        plt.scatter(subset[\"Height (m)\"], subset[\"Fall Time (s)\"], color=colors[i], label=f\"Training Data (Subset {i + 1})\", alpha=0.6)\n",
    "\n",
    "        # Plot the test data points (fall times vs heights)\n",
    "        plt.scatter(test_sample[\"Height (m)\"], test_sample[\"Fall Time (s)\"], marker='x', color='green', label=f\"Test Data (Subset {i + 1})\", s=150)\n",
    "\n",
    "        # Plot the predicted fall times (using the same heights as the test sample)\n",
    "        plt.scatter(test_sample[\"Height (m)\"], y_pred, marker='x', color='red', label=f\"Predicted Data (Subset {i + 1})\", s=150)\n",
    "\n",
    "    # Calculate and display the overall MSE across all subsets\n",
    "    plt.title(f\"Training, Test, and Predicted Fall Times (Fall Time vs Height)\")\n",
    "    plt.xlabel(\"Height (m)\")\n",
    "    plt.ylabel(\"Fall Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    overall_mse = compute_mse(actual_values, predicted_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f572a54",
   "metadata": {},
   "source": [
    "### 1. Visualize test and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6f2d7",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can decide the number of subsets used in this section by changing the parameter ``num_splits``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2df2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = None # CHANGE num_splits variable from None to natural numbers in the interval [1, 21]\n",
    "\n",
    "# DO NOT CHANGE THE CODE BELOW\n",
    "check_value(num_splits, \"num_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1672a3d-fc50-4403-ba4e-798f57acb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test and training data and visualize it in a scatter plot\n",
    "selected_samples = visualize_all_without_predictions(df, num_splits, seed=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42addd4e",
   "metadata": {},
   "source": [
    "<u>TASK:</u> In the code cell below you can change the bandwidth parameter to increase or decrease the dependency \\\n",
    "of the predictions on the distances of other data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd9a7d-d9d5-4332-8a17-33632e1ce44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = None # CHANGE bandwidth variable from None to rational numbers in the interval [0, 100]\n",
    "\n",
    "# DO NOT CHANGE THE CODE BELOW\n",
    "check_value(bandwidth, \"bandwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e8d1c",
   "metadata": {},
   "source": [
    "### 2. Visualize predictions with the specified bandwidth parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af443d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distances of training data to test data points and the importance weight matrix, visualize the predictions\n",
    "visualize_all_with_predictions(df, num_splits, selected_samples, bandwidth = bandwidth) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
